{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppwDtMwygjsF"
      },
      "source": [
        "# Notebook API Python Spark DataFrame - Graphs with DataFrames- ESILV 2025\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmmaq28FhQPQ"
      },
      "source": [
        "## Preparation\n",
        "*   ***Check that computing resources*** are allocated to your notebook if it is\n",
        "connected (see disk RAM indicated at top right). If not, click on the connect button to obtain resources.\n",
        "\n",
        "*   ***Create the directory*** to store the necessary files on your google\n",
        "drive (give the notebook permission to access your drive when requested). *Adjust the name of your folder* : **MyDrive/ens/esilv/data/**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeAagUyViAQ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ea5f197-5c60-4236-e1e9-6ab06d407c01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data.csv.gz',\n",
              " 'meta.csv.gz',\n",
              " 'airports.csv',\n",
              " 'airlines.csv',\n",
              " '188591317_T_ONTIME.csv.gz']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "drive_dir = \"/content/drive/MyDrive/ens/esilv/data/\"\n",
        "os.makedirs(drive_dir, exist_ok=True)\n",
        "os.listdir(drive_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovdV_CzOjPFg"
      },
      "source": [
        "***Install pyspark and findspark:***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bM95ARs2jkAJ"
      },
      "outputs": [],
      "source": [
        "!pip install -q pyspark\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArRvZCfJs9Tm"
      },
      "source": [
        "***Install GraphFrames :***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6WZ7oreHwBT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfbda3b8-86a1-4e5e-bb74-bffd2d78c078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m153.6/154.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q graphframes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -L -o \"/usr/local/lib/python3.10/dist-packages/pyspark/jars/graphframes-0.8.3-spark3.5-s_2.12.jar\" http://dl.bintray.com/spark-packages/maven/graphframes/graphframes/0.8.3-spark3.5-s_2.12/graphframes-0.8.3-spark3.5-s_2.12.jar"
      ],
      "metadata": {
        "id": "ByLJaYH8pZ1n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b3a393c-ad23-4838-e52a-34300bf61c6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   164  100   164    0     0   1123      0 --:--:-- --:--:-- --:--:--  1131\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0Warning: Failed to create the file \n",
            "Warning: /usr/local/lib/python3.10/dist-packages/pyspark/jars/graphframes-0.8.3\n",
            "Warning: -spark3.5-s_2.12.jar: No such file or directory\n",
            "100   146  100   146    0     0    244      0 --:--:-- --:--:-- --:--:--   705\n",
            "curl: (23) Failure writing output to destination\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwEv6d4m1AX0"
      },
      "source": [
        "***Start the spark session:***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmyc4bS61DO9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# !find /usr/local -name \"pyspark\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/usr/local/lib/python3.12/dist-packages/pyspark\"\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Np0ufet11H_9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33f34f34-a376-4b42-cef9-c2e8a4daf55d",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "session started, its id is  local-1765487329930\n"
          ]
        }
      ],
      "source": [
        "# Main imports\n",
        "import findspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf\n",
        "\n",
        "# for dataframe and udf\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from datetime import *\n",
        "\n",
        "# initialise environment variables for spark\n",
        "findspark.init()\n",
        "\n",
        "# Start spark session\n",
        "# --------------------------\n",
        "def start_spark():\n",
        "  local = \"local[*]\"\n",
        "  appName = \"TP\"\n",
        "\n",
        "  gf = \"graphframes:graphframes:0.8.3-spark3.5-s_2.12\"\n",
        "\n",
        "  configLocale = SparkConf().setAppName(appName).setMaster(local).\\\n",
        "  set(\"spark.executor.memory\", \"6G\").\\\n",
        "  set(\"spark.driver.memory\",\"6G\").\\\n",
        "  set(\"spark.sql.catalogImplementation\",\"in-memory\").\\\n",
        "  set(\"spark.jars.packages\", gf)\n",
        "\n",
        "  spark = SparkSession.builder.config(conf = configLocale).getOrCreate()\n",
        "  sc = spark.sparkContext\n",
        "  sc.setLogLevel(\"ERROR\")\n",
        "\n",
        "  spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\",\"-1\")\n",
        "\n",
        "  # Adjust the query execution environment to the size of the cluster (4 cores)\n",
        "  spark.conf.set(\"spark.sql.shuffle.partitions\",\"4\")\n",
        "  print(\"session started, its id is \", sc.applicationId)\n",
        "  return spark\n",
        "spark = start_spark()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qO_BThNgxjY"
      },
      "source": [
        "## Synthetic data on music\n",
        "\n",
        "##### Data description:\n",
        "\n",
        "- File **data.csv**: contains information on songs (trackId) performed by artists (artistId) and listened to by users (userId) on a date given by a timestamp. Contains 260664 lines.\n",
        "- File **meta.csv**: contains the names (field 'Name') of the songs if type==track or of the artists if type==artist.\n",
        "  Artist' is the name of the artist performing the song if type==track or the name of the artist (same value as 'Name') if track==artist. Id' is the identifier of a song or artist. Contains 44319 lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-LLC8Ct1vgj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60c1879b-7872-47b0-e8b3-8afd2c8182d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URL for data:  https://nuage.lip6.fr/s/LqD9N23kxrfHopr\n"
          ]
        }
      ],
      "source": [
        "# URL of the folder containing useful data files (data.csv and meta.csv)\n",
        "# ---------------------------------------------------------------------------\n",
        "# if you have problems downloading datasets, go directly to the URL below\n",
        "PUBLIC_DATASET_URL = \"https://nuage.lip6.fr/s/LqD9N23kxrfHopr\"\n",
        "PUBLIC_DATASET=PUBLIC_DATASET_URL + \"/download?path=\"\n",
        "\n",
        "print(\"URL for data: \", PUBLIC_DATASET_URL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ga5tqvIhDFI"
      },
      "source": [
        "## Read data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fart-YsU2c_Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "145f6cad-3fb5-4ba5-ee18-de8e9640d76d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data.csv.gz is already stored\n",
            "meta.csv.gz is already stored\n",
            "Files downloaded:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data.csv.gz',\n",
              " 'meta.csv.gz',\n",
              " 'airports.csv',\n",
              " 'airlines.csv',\n",
              " '188591317_T_ONTIME.csv.gz']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import os\n",
        "from urllib import request\n",
        "\n",
        "import os\n",
        "from urllib import request\n",
        "\n",
        "def load_file(file,dir):\n",
        "  if(os.path.isfile(drive_dir+file)):\n",
        "    print(file, \"is already stored\")\n",
        "  else:\n",
        "    url = PUBLIC_DATASET + \"/\"+ dir + \"/\" + file\n",
        "    print(\"downloading from URL: \", url, \"save in : \" + drive_dir   + file)\n",
        "    request.urlretrieve(url , drive_dir + file)\n",
        "\n",
        "load_file(\"data.csv.gz\", \"musique\")\n",
        "load_file(\"meta.csv.gz\", \"musique\")\n",
        "\n",
        "# Liste des fichiers téléchargés\n",
        "print(\"Files downloaded:\")\n",
        "os.listdir(drive_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The folder containing the imported csv files:\n",
        "DATASET_DIR=\"/content/drive/MyDrive/ens/esilv/data\""
      ],
      "metadata": {
        "id": "G_HVD_uAX4TC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzcaU7Qd35sM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9e8c4ef-32bf-4983-a8e1-3a4ef76827f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading file:  /content/drive/MyDrive/ens/esilv/data/data.csv.gz\n"
          ]
        }
      ],
      "source": [
        "#==============\n",
        "# Data\n",
        "#==============\n",
        "schema = \"\"\"\n",
        "          userId LONG,\n",
        "          trackId LONG,\n",
        "          artistId LONG,\n",
        "          timestamp LONG\n",
        "        \"\"\"\n",
        "print(\"Reading file: \", DATASET_DIR+\"/data.csv.gz\")\n",
        "data = spark.read.format(\"csv\").option(\"header\", \"true\").schema(schema) \\\n",
        "            .load(DATASET_DIR+\"/data.csv.gz\").persist()\n",
        "#data.show(5)\n",
        "#data.count() #260664\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwrqpoZY-gJd"
      },
      "outputs": [],
      "source": [
        "#==============\n",
        "# Data description\n",
        "#==============\n",
        "schema = \"\"\"\n",
        "          type STRING,\n",
        "          Name STRING,\n",
        "          Artist STRING,\n",
        "          Id LONG\n",
        "        \"\"\"\n",
        "print(\"reading file: \", DATASET_DIR+\"/meta.csv.gz\")\n",
        "meta = spark.read.format(\"csv\").option(\"header\", \"true\").schema(schema) \\\n",
        "            .load(DATASET_DIR+\"/meta.csv.gz\").persist()\n",
        "\n",
        "meta.show(5)\n",
        "meta.count() #44319"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVSOPqDs4QZa"
      },
      "source": [
        "## Build the graph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyfdVl7t4RdR"
      },
      "outputs": [],
      "source": [
        "#function used to calculate arc weights\n",
        "#df: dataframe, source: name of column containing arc source nodes\n",
        "#weight: initial weight before normalization, n: maximum number of arcs to keep for each source\n",
        "from pyspark.sql.functions import row_number, sum\n",
        "from pyspark.sql import Window\n",
        "\n",
        "def compute_weight(df, source, weight, n):\n",
        "\n",
        "    window = Window.partitionBy(source).orderBy(col(weight).desc())\n",
        "\n",
        "    filterDF = df.withColumn(\"row_number\", row_number().over(window)) \\\n",
        "        .filter(col(\"row_number\") <= n) \\\n",
        "        .drop(col(\"row_number\"))\n",
        "\n",
        "    tmpDF = filterDF.groupBy(col(source)).agg(sum(col(weight)).alias(\"sum_\" + weight))\n",
        "\n",
        "    finalDF = filterDF.join(tmpDF, source, \"inner\") \\\n",
        "        .withColumn(\"norm_\" + weight, col(weight) / col(\"sum_\" + weight)) \\\n",
        "        .cache()\n",
        "    return finalDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xx4_sb9B5eb_"
      },
      "source": [
        " ### Building weighted links between users and songs\n",
        "* Build a userTrack DataFrame from data to store arcs between users and songs. For each user (userId) we add an arc to a song (trackId) with a weight equal to the total number of times the user has listened to the song. Use the compute_weight function to save for each user the 100 songs with the highest weight and normalize the weights of the saved arcs.   \n",
        "    \n",
        "* Display the result: keep only the arcs with the 5 highest possible weight values (use the dense_rank() function and the over(window) window). Display 20 rows of the result, sorting in descending order of weights, then in ascending order of userId and trackId."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjALrImW5kcQ"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, rank\n",
        "\n",
        "#number of times each user listens to a track\n",
        "userTrack = ...\n",
        "\n",
        "#calculate final weight using compute_weight\n",
        "userTrack = ...\n",
        "\n",
        "window = Window.partitionBy(\"userId\").orderBy(col(\"norm_count\").desc())\n",
        "\n",
        "userTrackList = userTrack.withColumn(\"position\", dense_rank().over(window))\\\n",
        "       ...\\\n",
        "       .take(20)\n",
        "\n",
        "\n",
        "for val in userTrackList:\n",
        "   print(\"%s %s %s\" % val)\n",
        "\n",
        "\n",
        "userTrack.count() # 210675\n",
        "userTrack.printSchema()\n",
        "#root\n",
        "# |-- userId: long (nullable = true)\n",
        "# |-- trackId: long (nullable = true)\n",
        "# |-- norm_count: double (nullable = true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9doji9OFfDZ"
      },
      "source": [
        "### Building weighted links between users and artists\n",
        "* Build a userArtist DataFrame from data to store arcs between users and artists. For each user (userId) we add an arc to an artist (artistId) with a weight equal to the total number of times the user has listened to songs by this artist. Use the compute_weight function to keep for each user at most 100 artists with the highest weight and normalize the weights of the arcs kept.   \n",
        "    \n",
        "* Display the result: keep only the arcs with the 5 highest possible weight values (use the dense_rank() function and the over(window) window).\n",
        "   Display 20 rows of the result, sorting in descending order of weights, then in ascending order of userId and artistId."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hnt69ccFnLj"
      },
      "outputs": [],
      "source": [
        "# weight=number of times a user has listened to tracks by this artist\n",
        "# group data by userId and artistId\n",
        "userArtist = ...\n",
        "\n",
        "#calculate final weight using compute_weight\n",
        "userArtist = ....persist()\n",
        "\n",
        "window = Window.partitionBy(\"userId\").orderBy(col(\"norm_count\").desc())\n",
        "\n",
        "userArtistList = userArtist.withColumn(\"position\", dense_rank().over(window))\\\n",
        "                ...\\\n",
        "                .take(20)\n",
        "\n",
        "for val in userArtistList:\n",
        "   print(\"%s %s %s %s\" % val)\n",
        "\n",
        "userArtist.count() #178419\n",
        "userArtist.printSchema()\n",
        "#root\n",
        "# |-- userId: long (nullable = true)\n",
        "# |-- artistId: long (nullable = true)\n",
        "# |-- norm_count: double (nullable = true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNRUOI5eF0_7"
      },
      "source": [
        "### Building weighted links between artists and songs\n",
        "* Build an artistTrack DataFrame from data to store arcs between artists and songs. For each artist (artistId), add an arc to a song (trackId) with a weight equal to the total number of times that artist's song has been listened to by all users. Use the compute_weight function to keep for each artist at most 100 songs with the highest weight and normalize the weights of the arcs kept.   \n",
        "    \n",
        "* Display the result: keep only the arcs with the 5 highest possible weight values (use the dense_rank() function and the over(window) window).\n",
        "   Display 20 rows of the result, sorting in descending order of weights, then in ascending order of artistId and trackId."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rB31s1wrF5hS"
      },
      "outputs": [],
      "source": [
        "# arc weight: number of times an artist's track has been listened to by all users\n",
        "\n",
        "artistTrack = ...\n",
        "\n",
        "artistTrack = ...persist()\n",
        "\n",
        "window = Window.partitionBy(\"artistId\").orderBy(col(\"norm_count\").desc())\n",
        "\n",
        "\n",
        "artistTrackList = artistTrack.withColumn(\"position\", dense_rank().over(window))\\\n",
        "       ...\\\n",
        "       .take(20)\n",
        "\n",
        "for val in artistTrackList:\n",
        "   print(\"%s %s %s\" % val)\n",
        "\n",
        "artistTrack.count() # 35408\n",
        "artistTrack.printSchema()\n",
        "#root\n",
        "# |-- artistId: long (nullable = true)\n",
        "# |-- trackId: long (nullable = true)\n",
        "# |-- norm_count: double (nullable = true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyZIuE1yGDmb"
      },
      "source": [
        "### Building weighted links between songs\n",
        "* Build a trackTrack DataFrame from data to store arcs between songs. An arc exists between trackId1 and trackId2 if at least one user has listened to both songs. The total weight of an arc between trackId1 and trackId2 is the total number of users who have listened to both trackId1 and trackId2 within 10 minutes (note that the graph is undirected, trackTrack contains both an entry for (trackId1, trackId2) and an entry for (trackId2, trackId1)).   \n",
        "Use the compute_weight function to keep for each song at most 100 songs with the highest weight and normalize the weights kept.   \n",
        "    \n",
        "* Display the result: keep only the arcs with the 5 highest possible weight values (use the dense_rank() function and the over(window) window).\n",
        "   Display 20 rows of the result, sorting in descending order of weights, then in ascending order of artistId and trackId."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQC9fiRnGG71"
      },
      "outputs": [],
      "source": [
        "from datetime import *\n",
        "from pyspark.sql.functions import abs\n",
        "\n",
        "# Build trackId pairs listened to by the same user\n",
        "...\n",
        "\n",
        "\n",
        "#for each trackId pair, the number of users who listened to them together\n",
        "...\n",
        "\n",
        "#calculate final weight using compute_weight\n",
        "trackTrack = ....persist()\n",
        "\n",
        "window = Window.partitionBy(\"trackId\").orderBy(col(\"norm_count\").desc())\n",
        "\n",
        "trackTrackList = trackTrack.withColumn(\"position\", dense_rank().over(window))\\\n",
        "       ...\\\n",
        "       .take(20)\n",
        "\n",
        "for val in trackTrackList:\n",
        "   print(\"%s %s %s\" % val)\n",
        "\n",
        "trackTrack.count() #136257\n",
        "trackTrack.printSchema()\n",
        "#root\n",
        "# |-- trackId: long (nullable = true)\n",
        "# |-- track1: long (nullable = true)\n",
        "# |-- norm_count: double (nullable = true)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6szO3vKGWbF"
      },
      "source": [
        "## Building the final graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCB3eOxcGcOs"
      },
      "source": [
        "Build a graph DataFrame to store all the nodes and links calculated above. The dataframe will contain a 'source' column (source node identifier), a 'destination' column and a 'weight' column.\n",
        "The 'source' and 'dest' columns contain both user, song and artist IDs. The 'weight' column contains the arc weights calculated from the previous weights, multiplied by the following coefficients:\n",
        "* Links user->artist: 0.5\n",
        "* Links user->track: 0.5\n",
        "* Links track->track: 1\n",
        "* Links artist->track: 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EI2e8fyeGqZa"
      },
      "outputs": [],
      "source": [
        "...\n",
        "\n",
        "graph = ....persist()\n",
        "graph.count() #560759\n",
        "graph.printSchema()\n",
        "#root\n",
        "# |-- source: long (nullable = true)\n",
        "# |-- dest: long (nullable = true)\n",
        "# |-- poids: double (nullable = true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuI7LKmxHZi9"
      },
      "source": [
        "### Computing song recommendations with PPR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssDVSPK4HadQ"
      },
      "source": [
        "Using the Personalized PageRank calculation, recommend to the user with ID 10 the songs he hasn't listened to. Recall the formula for updating the recommendation score at each iteration of the calculation:\n",
        "\n",
        "x[i] = (1-d) * v[i] + d* sum(xant[j]*weight[j][i])\n",
        "\n",
        "    - weight[j][i] : weight of arc between j and i\n",
        "    - v[i]: personalization value, v[10]=1 and v[i]=0 if i !=10\n",
        "    - xant[j] : score value of node j at previous iteration (x0[10]=1-d and x0[i]=0 if i !=10)\n",
        "\n",
        "We consider d=0.85 and perform the calculation for 5 iterations (maxiter=5)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck2fUXL_HkaP"
      },
      "source": [
        "### Computation of the recommendation vector x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLIyw3uqHrLg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "user = 10\n",
        "d=0.85\n",
        "maxiter = 5\n",
        "# Build the initial importance vector\n",
        "x0  = spark.createDataFrame(pd.DataFrame([(user,1)], columns=[\"id\",\"rank\"]))\n",
        "\n",
        "print(\"Initial importance\")\n",
        "x0.show()\n",
        "\n",
        "x = x0\n",
        "for iter in range(maxiter) :\n",
        "\n",
        "    nextx = ...\n",
        "\n",
        "    x = ...\n",
        "\n",
        "\n",
        "x = x.persist()\n",
        "print(\"Final importance\")\n",
        "x.orderBy(col(\"rank\").desc()).show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```\n",
        "# Result:\n",
        "\n",
        "Initial importance\n",
        "+---+----+\n",
        "| id|rank|\n",
        "+---+----+\n",
        "| 10|   1|\n",
        "+---+----+\n",
        "\n",
        "Final importance\n",
        "+------+--------------------+\n",
        "|    id|                rank|\n",
        "+------+--------------------+\n",
        "|    10| 0.15000000000000002|\n",
        "|839649| 0.04804175618196067|\n",
        "|828318| 0.04146198925647322|\n",
        "|960353|0.038525490989983785|\n",
        "|960214|0.037971415875705114|\n",
        "|955486|  0.0344843898841708|\n",
        "|855194| 0.02795106930767553|\n",
        "|958924|0.025842628563049787|\n",
        "|801772|0.025352473837919276|\n",
        "|807650|0.021943036028298665|\n",
        "|823737|0.021943036028298665|\n",
        "|901153| 0.02054042805530674|\n",
        "|984123|           0.0159375|\n",
        "|972772|           0.0159375|\n",
        "|949111|0.014455449194238305|\n",
        "|955858|0.014255786458993664|\n",
        "|849768|0.009970758893824672|\n",
        "|824440| 0.00921446638329685|\n",
        "|969620|          0.00796875|\n",
        "|941064|       0.00737109375|\n",
        "+------+--------------------+\n",
        "only showing top 20 rows\n",
        "```"
      ],
      "metadata": {
        "id": "QtmHHKMrwH2m"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjEpi7tAuGJk"
      },
      "source": [
        "### Building a list of recommended songs that the user hasn't listened to.\n",
        "Display the 10 most recommended songs that the user hasn't listened to, with their score\n",
        "previously calculated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vS3ikClIuKxx"
      },
      "outputs": [],
      "source": [
        "\n",
        "...\n",
        "tracksrec = ...\n",
        "\n",
        "\n",
        "window = Window.orderBy(col(\"rank\").desc())\n",
        "\n",
        "\n",
        "tracksreckList = tracksrec.withColumn(\"position\", dense_rank().over(window))\\\n",
        "       ....take(10)\n",
        "\n",
        "for val in tracksreckList:\n",
        "   print(\"%s %s %s %s\" % val)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Result:\n",
        "\n",
        "949111 0.01445053790893624 Hah Heh Hah Artist: Vaya Con Dios\n",
        "955858 0.014255786458993664 Legz Artist: Jaffa\n",
        "849768 0.00997048013738679 martin Artist: Dani Martin\n",
        "803682 0.005324306187499334 Every Morning Artist: Sugar Ray\n",
        "926933 0.004131316860041196 Who You Want Artist: Qulinez\n",
        "834413 0.004061852931960852 Bluebells Artist: Patrick Wolf\n",
        "855373 0.004002950559461102 Marat Artist: Eminem\n",
        "816283 0.003639860297309027 Requiem Pour Un Fou Artist: Johnny Hallyday\n",
        "964744 0.0032711386413310814 Perdono Artist: Tiziano Ferro\n",
        "893155 0.0030469417163617577 Quit   ft Ariana Grande Artist: Cashmere Cat[texte du lien](https://)\n",
        "```"
      ],
      "metadata": {
        "id": "8Bp7IZ7lwwiz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBCj7dGTwF1V"
      },
      "source": [
        "## Computing triangles\n",
        "\n",
        "Implement the various steps of the improved algorithm for calculating triangles\n",
        "presented in class on the trackTrack graph constructed earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSTjGZ1SwHCS"
      },
      "outputs": [],
      "source": [
        "# Define a function that takes as argument a list of users sorted in ascending order (users) and returns a list of ordered pairs of users\n",
        "# Attention: chaque couple [a,b] est représenté par une chaîne de caractères \"[a,b]\"\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "def parse_string(users):\n",
        "    ...\n",
        "    return results\n",
        "\n",
        "parse_string_udf = udf(parse_string, ArrayType(StringType()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtl6tIWwwP9N"
      },
      "outputs": [],
      "source": [
        "# Implementing the triangle computation algorithm\n",
        "\n",
        "from pyspark.sql.functions import collect_list, sort_array, explode\n",
        "\n",
        "#Map1 - see slides\n",
        "#Consider only ordered pairs (trackId, track1) (trackId < track1)\n",
        "trackOrd = ...\n",
        "\n",
        "#Reduce 1 - slides: Build for each trackId the list of ordered pairs of its neighbors\n",
        "# a) group lines by trackId by building a list of neighbors (>trackId) sorted in ascending order (use sort_array)\n",
        "neighbors = ...\n",
        "\n",
        "\n",
        "# b) use the function defined above to return the list of possible pairs of neighbors\n",
        "couples=...\n",
        "\n",
        "# Map2 + Reduce 2 - see slides\n",
        "# consider only those rows where the neighbor pairs previously constructed also exist in the graph (join)\n",
        "from pyspark.sql.functions import concat, lit, count, desc\n",
        "liste = ...\n",
        "\n",
        "# Compute the number of triangles for each user and sort the result by decreasing number of triangles.\n",
        "triangles = ...\n",
        "\n",
        "triangles.count() #7156\n",
        "\n",
        "triangles.orderBy(col(\"nb_triangles\").desc()).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Result:\n",
        "\n",
        "+-------+------------+\n",
        "|trackid|nb_triangles|\n",
        "+-------+------------+\n",
        "| 800288|        1161|\n",
        "| 808082|        1068|\n",
        "| 805688|         925|\n",
        "| 806854|         917|\n",
        "| 815388|         875|\n",
        "| 825174|         854|\n",
        "| 831005|         762|\n",
        "| 805959|         656|\n",
        "| 798800|         650|\n",
        "| 798517|         636|\n",
        "| 799541|         625|\n",
        "| 801571|         595|\n",
        "| 846624|         592|\n",
        "| 841340|         585|\n",
        "| 810775|         574|\n",
        "| 844296|         566|\n",
        "| 802599|         565|\n",
        "| 811513|         554|\n",
        "| 858904|         549|\n",
        "| 813969|         524|\n",
        "+-------+------------+\n",
        "only showing top 20 rows\n",
        "```"
      ],
      "metadata": {
        "id": "aTH41foIxjOz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyNh3sPh8Kx_"
      },
      "source": [
        "## Single Source Shortest Paths\n",
        "\n",
        "Implement the parallel algorithm for calculating the shortest path from a fixed origin to any destination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJqQCdY9y_zc"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "import pandas as pd\n",
        "\n",
        "from graphframes import GraphFrame\n",
        "from graphframes.lib import AggregateMessages as AM\n",
        "\n",
        "origin=10\n",
        "\n",
        "#Gather all nodes in a single DF\n",
        "vertices = ...\n",
        "# Vector of distances of all nodes from origin\n",
        "distances = vertices.withColumn(\"distance\", F.when(vertices[\"id\"] == origin, 0).otherwise(float(\"inf\")))\n",
        "active  = spark.createDataFrame(pd.DataFrame([(origin,0)], columns=[\"idA\",\"distanceA\"]))\n",
        "\n",
        "distances= AM.getCachedDataFrame(distances)\n",
        "\n",
        "i=0\n",
        "while active.first():\n",
        "  ...\n",
        "  distances = ...\n",
        "\n",
        "  active=AM.getCachedDataFrame(...)\n",
        "  distances=AM.getCachedDataFrame(...)\n",
        "\n",
        "\n",
        "print(\"Final distances:\")\n",
        "distances.filter(col(\"distance\")!=float('inf')).orderBy(col(\"distance\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Result:\n",
        "\n",
        "Final distances:\n",
        "+------+--------+\n",
        "|    id|distance|\n",
        "+------+--------+\n",
        "|    10|     0.0|\n",
        "|839649| 0.03125|\n",
        "|807650| 0.03125|\n",
        "|823737| 0.03125|\n",
        "|824440| 0.03125|\n",
        "|828318| 0.03125|\n",
        "|901153| 0.03125|\n",
        "|801772| 0.03125|\n",
        "|855194| 0.03125|\n",
        "|934050| 0.03125|\n",
        "|941064| 0.03125|\n",
        "|943645| 0.03125|\n",
        "|955486| 0.03125|\n",
        "|956604| 0.03125|\n",
        "|958924| 0.03125|\n",
        "|960214| 0.03125|\n",
        "|960353| 0.03125|\n",
        "|970381| 0.03125|\n",
        "|976111| 0.03125|\n",
        "|983989| 0.03125|\n",
        "+------+--------+\n",
        "only showing top 20 rows\n",
        "```"
      ],
      "metadata": {
        "id": "HViLMe68yjwb"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}