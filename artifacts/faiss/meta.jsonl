{"id": "MANIFEST.json#p0#c0", "doc_id": "MANIFEST.json", "page": null, "text": "{\n \"files\": [\n \"data/raw/code/01_utils_spark_session.py\",\n \"data/raw/code/02_dataframe_basics.py\",\n \"data/raw/code/03_joins_and_aggs.py\",\n \"data/raw/code/04_graphframes_build_graph.py\",\n \"data/raw/code/05_graphframes_bfs.py\",\n \"data/raw/code/06_graphframes_pagerank.py\",\n \"data/raw/code/07_graphframes_connected_components.py\",\n \"data/raw/code/08_graphframes_motifs_triangles.py\",\n \"data/raw/code/09_personalized_pagerank_notes_and_skeleton.py\",\n \"data/raw/code/09_triangle_count_dataframe_only.py\",\n \"data/raw/code/10_personalized_pagerank_reco.py\",\n \"data/raw/code/10_similarity_jaccard_cosine_df.py\",\n \"data/raw/meta/00_topic_map.md\",\n \"data/raw/meta/01_study_plan.md\",\n \"data/raw/meta/02_exam_strategy_closed_book.md\",\n \"data/raw/meta/03_glossary.md\",\n \"data/raw/meta/04_modes_and_outputs.md\",\n \"data/raw/meta/05_fallback_policy.md\",\n \"data/raw/meta/topic_map.json\",\n \"data/raw/notes/01_exam_syll"}
{"id": "MANIFEST.json#p0#c1", "doc_id": "MANIFEST.json", "page": null, "text": ".md\",\n \"data/raw/meta/04_modes_and_outputs.md\",\n \"data/raw/meta/05_fallback_policy.md\",\n \"data/raw/meta/topic_map.json\",\n \"data/raw/notes/01_exam_syllabus.md\",\n \"data/raw/notes/02_spark_core_rdd.md\",\n \"data/raw/notes/03_spark_dataframes.md\",\n \"data/raw/notes/04_graph_theory_basics.md\",\n \"data/raw/notes/05_graph_algorithms_math.md\",\n \"data/raw/notes/06_graphframes_api.md\",\n \"data/raw/notes/07_graph_dataframe_algorithms.md\",\n \"data/raw/notes/08_recommendation_ppr_workflow.md\",\n \"data/raw/notes/09_exam_question_patterns.md\",\n \"data/raw/notes/10_revision_strategy.md\",\n \"data/raw/notes/extras/11_dataframe_optimization_and_performance.md\",\n \"data/raw/notes/extras/12_graph_algorithms_exam_cheatsheet.md\",\n \"data/raw/notes/extras/13_common_exam_pitfalls.md\",\n \"data/raw/qbank/coding_tasks.json\",\n \"data/raw/qbank/math_exercises.json\",\n \"data/raw/qbank/mcq.json\",\n \"data/raw/qbank/short_answer.json\","}
{"id": "MANIFEST.json#p0#c2", "doc_id": "MANIFEST.json", "page": null, "text": "alls.md\",\n \"data/raw/qbank/coding_tasks.json\",\n \"data/raw/qbank/math_exercises.json\",\n \"data/raw/qbank/mcq.json\",\n \"data/raw/qbank/short_answer.json\",\n \"data/raw/sources/ESILV_Dataframes_Graphs_2025_ETU.ipynb\",\n \"data/raw/sources/GraphFrames_class_notebook_2026.ipynb\",\n \"data/raw/sources/Lecture1-SparkDataFrames-eng-2025-2026.pdf\",\n \"data/raw/sources/Lecture2-graph-DataFrames-eng-25 (1).pdf\",\n \"data/raw/sources/Lecture3-GraphFrames-ESILV-25-26.pdf\",\n \"data/raw/sources/TP_CNAM_2026_ETU.ipynb\",\n \"data/raw/sources/construction-graphe-DataFrame-TME.pdf\"\n ],\n \"count\": 43\n}"}
{"id": "MANIFEST.txt#p0#c0", "doc_id": "MANIFEST.txt", "page": null, "text": "data/raw/code/01_utils_spark_session.py\ndata/raw/code/02_dataframe_basics.py\ndata/raw/code/03_joins_and_aggs.py\ndata/raw/code/04_graphframes_build_graph.py\ndata/raw/code/05_graphframes_bfs.py\ndata/raw/code/06_graphframes_pagerank.py\ndata/raw/code/07_graphframes_connected_components.py\ndata/raw/code/08_graphframes_motifs_triangles.py\ndata/raw/code/09_personalized_pagerank_notes_and_skeleton.py\ndata/raw/code/09_triangle_count_dataframe_only.py\ndata/raw/code/10_personalized_pagerank_reco.py\ndata/raw/code/10_similarity_jaccard_cosine_df.py\ndata/raw/meta/00_topic_map.md\ndata/raw/meta/01_study_plan.md\ndata/raw/meta/02_exam_strategy_closed_book.md\ndata/raw/meta/03_glossary.md\ndata/raw/meta/04_modes_and_outputs.md\ndata/raw/meta/05_fallback_policy.md\ndata/raw/meta/topic_map.json\ndata/raw/notes/01_exam_syllabus.md\ndata/raw/notes/02_spark_core_rdd.md\ndata/raw/notes/03_spark_dataframes.md\ndata/raw/n"}
{"id": "MANIFEST.txt#p0#c1", "doc_id": "MANIFEST.txt", "page": null, "text": "d\ndata/raw/meta/topic_map.json\ndata/raw/notes/01_exam_syllabus.md\ndata/raw/notes/02_spark_core_rdd.md\ndata/raw/notes/03_spark_dataframes.md\ndata/raw/notes/04_graph_theory_basics.md\ndata/raw/notes/05_graph_algorithms_math.md\ndata/raw/notes/06_graphframes_api.md\ndata/raw/notes/07_graph_dataframe_algorithms.md\ndata/raw/notes/08_recommendation_ppr_workflow.md\ndata/raw/notes/09_exam_question_patterns.md\ndata/raw/notes/10_revision_strategy.md\ndata/raw/notes/extras/11_dataframe_optimization_and_performance.md\ndata/raw/notes/extras/12_graph_algorithms_exam_cheatsheet.md\ndata/raw/notes/extras/13_common_exam_pitfalls.md\ndata/raw/qbank/coding_tasks.json\ndata/raw/qbank/math_exercises.json\ndata/raw/qbank/mcq.json\ndata/raw/qbank/short_answer.json\ndata/raw/sources/ESILV_Dataframes_Graphs_2025_ETU.ipynb\ndata/raw/sources/GraphFrames_class_notebook_2026.ipynb\ndata/raw/sources/Lecture1-SparkDataFrames-eng-"}
{"id": "MANIFEST.txt#p0#c2", "doc_id": "MANIFEST.txt", "page": null, "text": "w/sources/ESILV_Dataframes_Graphs_2025_ETU.ipynb\ndata/raw/sources/GraphFrames_class_notebook_2026.ipynb\ndata/raw/sources/Lecture1-SparkDataFrames-eng-2025-2026.pdf\ndata/raw/sources/Lecture2-graph-DataFrames-eng-25 (1).pdf\ndata/raw/sources/Lecture3-GraphFrames-ESILV-25-26.pdf\ndata/raw/sources/TP_CNAM_2026_ETU.ipynb\ndata/raw/sources/construction-graphe-DataFrame-TME.pdf"}
{"id": "README.md#p0#c0", "doc_id": "README.md", "page": null, "text": "# Exam RAG Dataset (Spark DataFrames + Graphs + GraphFrames)\nThis dataset reorganizes the provided course materials into **RAG-friendly files**: concise notes, runnable code templates, and a question bank (MCQ / short answers / coding tasks / math).\nIt is designed for a **closed-book written exam**, where questions may include:\n- Multiple choice (MCQ)\n- Short-answer / conceptual questions\n- Coding tasks (PySpark DataFrames, graph-as-dataframe, GraphFrames)\n## Folder structure\n- `data/raw/sources/`\n Original materials (PDF + notebooks) you uploaded.\n- `data/raw/notes/`\n Core notes and summaries (concepts, APIs, typical pitfalls, exam patterns).\n- `data/raw/meta/`\n High-level meta documents for abstract queries (syllabus outline, study plan, closed-book strategy, glossary, modes/output contract, fallback policy).\n- `data/raw/notes/extras/`\n Additional sheets: performance/optimization, algo"}
{"id": "README.md#p0#c1", "doc_id": "README.md", "page": null, "text": "closed-book strategy, glossary, modes/output contract, fallback policy).\n- `data/raw/notes/extras/`\n Additional sheets: performance/optimization, algorithm cheat-sheet, common mistakes.\n- `data/raw/code/`\n Minimal, runnable code templates that cover common exam-style tasks.\n- `data/raw/qbank/`\n Question bank in JSON: MCQ, short answers, coding tasks, and math exercises.\n## Suggested usage (RAG)\n1. Ingest all files under `data/raw/` into your RAG pipeline.\n2. Chunk primarily the Markdown notes and optionally code and JSON.\n3. For answering exam questions, retrieve relevant notes + code templates + similar questions.\n4. Return answers with citations (file + section/chunk id).\n## Quick sanity check\n- Notes: `data/raw/notes/*.md` and `data/raw/notes/extras/*.md`\n- Code: `data/raw/code/*.py`\n- QBank: `data/raw/qbank/*.json`"}
{"id": "01_utils_spark_session.py#p0#c0", "doc_id": "01_utils_spark_session.py", "page": null, "text": "\"\"\"SparkSession helper.\nUsage:\n from utils_spark_session import get_spark\n spark = get_spark(\"my-app\")\n\"\"\"\nfrom pyspark.sql import SparkSession\ndef get_spark(app_name: str = \"app\") -> SparkSession:\n return (\n SparkSession.builder\n .appName(app_name)\n .getOrCreate()\n )"}
{"id": "02_dataframe_basics.py#p0#c0", "doc_id": "02_dataframe_basics.py", "page": null, "text": "from pyspark.sql.functions import col\nfrom utils_spark_session import get_spark\ndef main():\n spark = get_spark(\"df-basics\")\n df = (spark.read\n .option(\"header\", True)\n .option(\"inferSchema\", True)\n .csv(\"data.csv\"))\n df.printSchema()\n out = (df.select(\"id\", \"name\", \"age\")\n .filter(col(\"age\") > 30)\n .orderBy(col(\"age\").desc()))\n out.show(20, truncate=False)\nif __name__ == \"__main__\":\n main()"}
{"id": "03_joins_and_aggs.py#p0#c0", "doc_id": "03_joins_and_aggs.py", "page": null, "text": "from pyspark.sql.functions import col, count, avg\nfrom utils_spark_session import get_spark\ndef main():\n spark = get_spark(\"joins-aggs\")\n movies = (spark.read.option(\"header\", True).option(\"inferSchema\", True)\n .csv(\"movies.csv\"))\n ratings = (spark.read.option(\"header\", True).option(\"inferSchema\", True)\n .csv(\"ratings.csv\"))\n # Example: average rating per movie\n stats = (ratings.groupBy(\"movieId\")\n .agg(count(\"rating\").alias(\"n\"), avg(\"rating\").alias(\"mean\")))\n # Join back to movie titles\n res = (movies.join(stats, on=\"movieId\", how=\"left\")\n .orderBy(col(\"n\").desc_nulls_last()))\n res.show(20, truncate=False)\nif __name__ == \"__main__\":\n main()"}
{"id": "04_graphframes_build_graph.py#p0#c0", "doc_id": "04_graphframes_build_graph.py", "page": null, "text": "\"\"\"Build a GraphFrame from vertices/edges DataFrames.\nRequires graphframes installed in the environment.\n\"\"\"\nfrom utils_spark_session import get_spark\nfrom pyspark.sql import functions as F\ndef main():\n spark = get_spark(\"graphframes-build\")\n # Minimal example graph\n v = spark.createDataFrame(\n [(\"a\", \"Alice\", 34), (\"b\", \"Bob\", 36), (\"c\", \"Charlie\", 30)],\n [\"id\", \"name\", \"age\"],\n )\n e = spark.createDataFrame(\n [(\"a\", \"b\", \"friend\"), (\"b\", \"c\", \"follow\"), (\"a\", \"c\", \"follow\")],\n [\"src\", \"dst\", \"relationship\"],\n )\n from graphframes import GraphFrame\n g = GraphFrame(v, e)\n g.vertices.show()\n g.edges.show()\n g.triplets.show(truncate=False)\nif __name__ == \"__main__\":\n main()"}
{"id": "05_graphframes_bfs.py#p0#c0", "doc_id": "05_graphframes_bfs.py", "page": null, "text": "from utils_spark_session import get_spark\ndef main():\n spark = get_spark(\"graphframes-bfs\")\n from graphframes import GraphFrame\n v = spark.createDataFrame(\n [(\"a\", \"Alice\", 34), (\"b\", \"Bob\", 36), (\"c\", \"Charlie\", 30), (\"d\", \"David\", 29)],\n [\"id\", \"name\", \"age\"],\n )\n e = spark.createDataFrame(\n [(\"a\", \"b\", \"friend\"), (\"b\", \"c\", \"follow\"), (\"c\", \"d\", \"follow\"), (\"a\", \"d\", \"follow\")],\n [\"src\", \"dst\", \"relationship\"],\n )\n g = GraphFrame(v, e)\n # Find shortest paths from Alice to nodes with age < 32\n paths = g.bfs(\"name = 'Alice'\", \"age < 32\", edgeFilter=\"relationship != 'friend'\", maxPathLength=3)\n paths.show(truncate=False)\nif __name__ == \"__main__\":n main()"}
{"id": "06_graphframes_pagerank.py#p0#c0", "doc_id": "06_graphframes_pagerank.py", "page": null, "text": "# GraphFrames PageRank template (PySpark)\n# Requires: from graphframes import GraphFrame\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nfrom graphframes import GraphFrame\nspark = SparkSession.builder.appName(\"graphframes-pagerank\").getOrCreate()\n# Example vertices and edges\nv = spark.createDataFrame([\n (\"a\",), (\"b\",), (\"c\",), (\"d\",)\n], [\"id\"])\ne = spark.createDataFrame([\n (\"a\",\"b\"), (\"b\",\"c\"), (\"c\",\"a\"), (\"c\",\"d\")\n], [\"src\",\"dst\"])\ng = GraphFrame(v, e)\npr = g.pageRank(resetProbability=0.15, maxIter=10)\npr.vertices.select(\"id\", col(\"pagerank\").alias(\"pr\")).orderBy(col(\"pr\").desc()).show()"}
{"id": "07_graphframes_connected_components.py#p0#c0", "doc_id": "07_graphframes_connected_components.py", "page": null, "text": "# Connected Components template (GraphFrames)\nfrom pyspark.sql import SparkSession\nfrom graphframes import GraphFrame\nspark = SparkSession.builder.appName(\"graphframes-cc\").getOrCreate()\nv = spark.createDataFrame([(1,), (2,), (3,), (4,), (5,)], [\"id\"])\ne = spark.createDataFrame([(1,2),(2,3),(4,5)], [\"src\",\"dst\"])\ng = GraphFrame(v, e)\ncc = g.connectedComponents() # returns (id, component)\ncc.orderBy(\"component\", \"id\").show()"}
{"id": "08_graphframes_motifs_triangles.py#p0#c0", "doc_id": "08_graphframes_motifs_triangles.py", "page": null, "text": "# Motif finding: triangles (GraphFrames)\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import expr\nfrom graphframes import GraphFrame\nspark = SparkSession.builder.appName(\"graphframes-motifs\").getOrCreate()\nv = spark.createDataFrame([(\"a\",), (\"b\",), (\"c\",)], [\"id\"])\ne = spark.createDataFrame([(\"a\",\"b\"),(\"b\",\"c\"),(\"c\",\"a\")], [\"src\",\"dst\"])\ng = GraphFrame(v, e)\nmotifs = g.find(\"(a)-[e1]->(b); (b)-[e2]->(c); (c)-[e3]->(a)\")\n# Optional: avoid duplicates by ordering ids\nmotifs = motifs.where(expr(\"a.id < b.id AND b.id < c.id\"))\nmotifs.selectExpr(\"a.id as a\", \"b.id as b\", \"c.id as c\").show()"}
{"id": "09_personalized_pagerank_notes_and_skeleton.py#p0#c0", "doc_id": "09_personalized_pagerank_notes_and_skeleton.py", "page": null, "text": "\"\"\"Personalized PageRank (PPR) skeleton.\nGraphFrames doesn't expose built-in PPR in all versions.\nCommon exam approach:\n- implement power iteration with teleport vector focusing on a seed node(s)\n- use DataFrame operations: join edges with current rank, aggregate, apply damping\nThis file provides a DataFrame-style skeleton.\n\"\"\"\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nspark = SparkSession.builder.appName(\"ppr\").getOrCreate()\n# edges: (src, dst)\nedges = spark.createDataFrame([\n (\"u1\", \"u2\"),\n (\"u2\", \"u3\"),\n (\"u3\", \"u1\"),\n], [\"src\", \"dst\"])\nseed = \"u1\"\nd = 0.85 # damping factor\nmax_iter = 10\n# out-degree\noutdeg = edges.groupBy(\"src\").agg(F.count(\"dst\").alias(\"outdeg\"))\n# initialize ranks\nverts = edges.select(F.col(\"src\").alias(\"id\")).union(edges.select(F.col(\"dst\").alias(\"id\"))).distinct()\nranks = verts.withColumn(\"rank\", F.when(F.col(\"id\") == seed, F.lit"}
{"id": "09_personalized_pagerank_notes_and_skeleton.py#p0#c1", "doc_id": "09_personalized_pagerank_notes_and_skeleton.py", "page": null, "text": "(F.col(\"src\").alias(\"id\")).union(edges.select(F.col(\"dst\").alias(\"id\"))).distinct()\nranks = verts.withColumn(\"rank\", F.when(F.col(\"id\") == seed, F.lit(1.0)).otherwise(F.lit(0.0)))\nfor _ in range(max_iter):\n # distribute rank along outgoing edges\n contrib = (edges.join(outdeg, on=\"src\", how=\"left\")\n .join(ranks.select(F.col(\"id\").alias(\"src\"), \"rank\"), on=\"src\", how=\"left\")\n .withColumn(\"contrib\", F.col(\"rank\") / F.col(\"outdeg\"))\n .groupBy(\"dst\").agg(F.sum(\"contrib\").alias(\"sum_contrib\"))\n )\n # new rank = d * incoming + (1-d) * personalization\n ranks = (verts.join(contrib, verts.id == contrib.dst, how=\"left\")\n .drop(\"dst\")\n .fillna({\"sum_contrib\": 0.0})\n .withColumn(\n \"rank\",\n F.lit(d) * F.col(\"sum_contrib\") + F.lit(1.0 - d) * F.when(F.col(\"id\") == seed, F.lit(1.0)).otherwise(F.lit(0.0))\n )\n .select(\"id\", \"rank\")\n )\nranks.orderBy(F.desc(\"rank\")).show()"}
{"id": "09_triangle_count_dataframe_only.py#p0#c0", "doc_id": "09_triangle_count_dataframe_only.py", "page": null, "text": "# Triangle count using only DataFrames (no GraphFrames)\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nspark = SparkSession.builder.appName(\"triangle-df\").getOrCreate()\n# edges: undirected, store with u < v to avoid duplicates\nedges = spark.createDataFrame([\n (\"a\",\"b\"), (\"b\",\"c\"), (\"c\",\"a\"), (\"c\",\"d\")\n], [\"u\",\"v\"])\n# Ensure ordering u < v\nedges = edges.select(\n col(\"u\").alias(\"u\"),\n col(\"v\").alias(\"v\")\n)\ne1 = edges.alias(\"e1\")\ne2 = edges.alias(\"e2\")\ne3 = edges.alias(\"e3\")\n# (a,b), (b,c), (a,c)\ntri = (e1.join(e2, col(\"e1.v\") == col(\"e2.u\"))\n .join(e3, (col(\"e1.u\") == col(\"e3.u\")) & (col(\"e2.v\") == col(\"e3.v\")))\n .select(col(\"e1.u\").alias(\"a\"), col(\"e1.v\").alias(\"b\"), col(\"e2.v\").alias(\"c\"))\n )\ntri.show()\nprint(\"triangle_count =\", tri.count())"}
{"id": "10_personalized_pagerank_reco.py#p0#c0", "doc_id": "10_personalized_pagerank_reco.py", "page": null, "text": "# Personalized PageRank (conceptual template)\n# GraphFrames has `personalizedPageRank` in some versions; if unavailable, simulate via random walk.\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, lit\nfrom graphframes import GraphFrame\nspark = SparkSession.builder.appName(\"ppr-reco\").getOrCreate()\nv = spark.createDataFrame([(\"u1\",), (\"u2\",), (\"i1\",), (\"i2\",), (\"i3\",)], [\"id\"])\ne = spark.createDataFrame([\n (\"u1\",\"i1\"), (\"u1\",\"i2\"), (\"u2\",\"i2\"), (\"u2\",\"i3\"),\n (\"i1\",\"i2\"), (\"i2\",\"i3\")\n], [\"src\",\"dst\"])\ng = GraphFrame(v, e)\n# If supported by your GraphFrames version:\n# ppr = g.personalizedPageRank(resetProbability=0.15, maxIter=10, sourceId=\"u1\")\n# ppr.vertices.orderBy(col(\"pagerank\").desc()).show()\nprint(\"Note: use g.pageRank for global PR or implement PPR if API is available.\")"}
{"id": "10_similarity_jaccard_cosine_df.py#p0#c0", "doc_id": "10_similarity_jaccard_cosine_df.py", "page": null, "text": "\"\"\"Similarity computations in DataFrames.\nExam patterns:\n- build user-item interactions\n- compute co-occurrence counts via self-join\n- Jaccard(u,v) = |Iu ∩ Iv| / |Iu ∪ Iv|\nThis shows Jaccard for item-item similarity.\n\"\"\"\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nspark = SparkSession.builder.appName(\"similarity\").getOrCreate()\n# interactions: (user, item)\nui = spark.createDataFrame([\n (\"u1\", \"i1\"), (\"u1\", \"i2\"),\n (\"u2\", \"i1\"), (\"u2\", \"i3\"),\n (\"u3\", \"i2\"), (\"u3\", \"i3\"),\n], [\"user\", \"item\"])\n# item -> users set size\nitem_deg = ui.groupBy(\"item\").agg(F.countDistinct(\"user\").alias(\"deg\"))\n# co-occurrence between items: users who interacted with both\npairs = (ui.alias(\"a\")\n .join(ui.alias(\"b\"), on=F.col(\"a.user\") == F.col(\"b.user\"))\n .where(F.col(\"a.item\") < F.col(\"b.item\"))\n .groupBy(F.col(\"a.item\").alias(\"i\"), F.col(\"b.item\").alias(\"j\"))\n .agg(F.countDistinc"}
{"id": "10_similarity_jaccard_cosine_df.py#p0#c1", "doc_id": "10_similarity_jaccard_cosine_df.py", "page": null, "text": " == F.col(\"b.user\"))\n .where(F.col(\"a.item\") < F.col(\"b.item\"))\n .groupBy(F.col(\"a.item\").alias(\"i\"), F.col(\"b.item\").alias(\"j\"))\n .agg(F.countDistinct(F.col(\"a.user\")).alias(\"inter\"))\n)\npairs = (pairs\n .join(item_deg.withColumnRenamed(\"item\",\"i\").withColumnRenamed(\"deg\",\"deg_i\"), on=\"i\")\n .join(item_deg.withColumnRenamed(\"item\",\"j\").withColumnRenamed(\"deg\",\"deg_j\"), on=\"j\")\n .withColumn(\"union\", F.col(\"deg_i\") + F.col(\"deg_j\") - F.col(\"inter\"))\n .withColumn(\"jaccard\", F.col(\"inter\") / F.col(\"union\"))\n)\npairs.orderBy(F.desc(\"jaccard\")).show(truncate=False)"}
{"id": "00_topic_map.md#p0#c0", "doc_id": "00_topic_map.md", "page": null, "text": "# Knowledge Map / Syllabus Outline (Spark DataFrames + Graphs + GraphFrames)\nThis file is designed for **high-level / abstract questions**:\n- \"How should I revise?\" / \"What's the outline?\" / \"What are the key topics?\"\n- \"What might be on the exam?\" / \"Give me a study plan\"\nIt is written as a **tree of topics**. Each node includes bilingual keywords to improve retrieval.\n---\n## A) Spark Fundamentals (基础)\n### A1) Spark architecture & execution\nKeywords: Driver, Executor, Job, Stage, Task, DAG, lazy evaluation, shuffle, partition\n- What runs on driver vs executors\n- Lazy evaluation and the execution plan (logical/physical)\n- Narrow vs wide transformations, shuffles, stages\n- Partitions and parallelism; how repartition/coalesce changes partitions\n### A2) RDD basics (if covered)\nKeywords: RDD, map, flatMap, reduceByKey, groupByKey, actions, transformations\n- When RDD is needed vs DataFrame\n- "}
{"id": "00_topic_map.md#p0#c1", "doc_id": "00_topic_map.md", "page": null, "text": "ns\n### A2) RDD basics (if covered)\nKeywords: RDD, map, flatMap, reduceByKey, groupByKey, actions, transformations\n- When RDD is needed vs DataFrame\n- Key-value operations and common pitfalls (groupByKey vs reduceByKey)\n### A3) Data sources & I/O\nKeywords: read, write, parquet, csv, json, schema, inferSchema\n- Explicit schema vs inferred schema\n- Partitioned datasets, save modes\n---\n## B) Spark DataFrames & SQL\n### B1) DataFrame API fundamentals\nKeywords: select, withColumn, filter/where, explode, when, cast, UDF\n- Column expressions (no Python loops in distributed operations)\n- Common patterns: parsing, normalization, explode arrays\n### B2) Aggregations & Window functions\nKeywords: groupBy, agg, count, sum, avg, distinct, window, over, partitionBy, orderBy\n- Aggregation semantics and null behavior\n- Window functions for ranking, moving averages, sessionization (if seen)\n### B3) Joins\nKey"}
{"id": "00_topic_map.md#p0#c2", "doc_id": "00_topic_map.md", "page": null, "text": "itionBy, orderBy\n- Aggregation semantics and null behavior\n- Window functions for ranking, moving averages, sessionization (if seen)\n### B3) Joins\nKeywords: join, inner, left, right, full, semi, anti, broadcast join\n- Join keys, duplicates, skew\n- When/why to broadcast; join order; common mistakes\n### B4) Performance / Optimization (closed-book exam often tests intuition)\nKeywords: cache/persist, checkpoint, explain, catalyst, tungsten, shuffle partitions\n- What triggers a shuffle\n- cache vs persist, when caching helps\n- How to read an `explain()` plan at a high level\n---\n## C) Graphs with DataFrames (Graphs-as-DataFrames)\n### C1) Graph modelling\nKeywords: vertices, edges, directed, undirected, weighted, adjacency list, incidence\n- Vertex table: id + attributes\n- Edge table: src, dst + attributes\n- Building a graph from interaction logs\n### C2) Typical tasks\nKeywords: degree, indegree, o"}
{"id": "00_topic_map.md#p0#c3", "doc_id": "00_topic_map.md", "page": null, "text": "able: id + attributes\n- Edge table: src, dst + attributes\n- Building a graph from interaction logs\n### C2) Typical tasks\nKeywords: degree, indegree, outdegree, path, triangle, motifs\n- Degree computation using aggregations\n- Motifs / patterns as joins\n---\n## D) GraphFrames (Spark)\n### D1) GraphFrames basics\nKeywords: GraphFrame, vertices, edges, caching, motif finding\n- Constructing a GraphFrame from two DataFrames\n- Persisting vertices/edges and graph\n### D2) Built-in algorithms\nKeywords: PageRank, ConnectedComponents, BFS, ShortestPaths, TriangleCount\n- Inputs/outputs of each algorithm (what columns appear)\n- Parameters to remember (maxIter, resetProbability, etc.)\n### D3) Motif finding\nKeywords: motifs, pattern language, g.find(\"(a)-[e]->(b)\")\n- How motif patterns map to returned DataFrames\n---\n## E) Recommendation / Similarity (if in materials)\n### E1) Similarity via DataFrames\nKeywo"}
{"id": "00_topic_map.md#p0#c4", "doc_id": "00_topic_map.md", "page": null, "text": ">(b)\")\n- How motif patterns map to returned DataFrames\n---\n## E) Recommendation / Similarity (if in materials)\n### E1) Similarity via DataFrames\nKeywords: Jaccard, cosine, co-occurrence, pairwise similarity\n- Build user-item sets, compute intersections/unions with joins\n### E2) Personalized PageRank (PPR)\nKeywords: random walk, restart probability, personalized pagerank\n- Intuition: bias walks from a seed node\n- Implementation idea: iterative propagation + normalization\n---\n## F) Exam-oriented outputs (what you should be able to do)\n### F1) Explain (简答题)\n- Define concepts precisely (e.g., shuffle, broadcast join)\n- Compare alternatives (RDD vs DF; cache vs persist)\n### F2) Coding (编程题)\n- Write DataFrame code for join/agg/window\n- Build GraphFrame and run BFS/PageRank/CC\n- Write motifs/triangle-style queries\n### F3) MCQ (选择题)\n- Recognize which operation causes shuffle\n- Identify correct A"}
{"id": "00_topic_map.md#p0#c5", "doc_id": "00_topic_map.md", "page": null, "text": "hFrame and run BFS/PageRank/CC\n- Write motifs/triangle-style queries\n### F3) MCQ (选择题)\n- Recognize which operation causes shuffle\n- Identify correct API usage and output schema\n---\n## Cross-reference hints\n- For Spark DF details: see `data/raw/notes/03_spark_dataframes.md`\n- For GraphFrames algorithms: see `data/raw/notes/06_graphframes_api_motifs.md` and `data/raw/notes/05_graph_algorithms_pagerank_bfs_cc.md`"}
{"id": "01_study_plan.md#p0#c0", "doc_id": "01_study_plan.md", "page": null, "text": "# Study Plan (Closed-book) - 10 days / 14 days\nChoose the plan based on your remaining time. Goal: be able to **write** code from memory and explain key concepts.\n## 10-day plan (2-3h/day)\nDay 1: Spark execution model + partition/shuffle mental model\n- Make a 1-page memory sheet: Job/Stage/Task, narrow vs wide, shuffle triggers\nDay 2: DataFrame core API\n- select/withColumn/filter, explode, when/cast\n- 5 mini-exercises (transform columns, parse strings, handle nulls)\nDay 3: Aggregations\n- groupBy + agg patterns, distinct, pivot (if relevant)\n- Build 10 query snippets you can rewrite from memory\nDay 4: Joins\n- inner/left, semi/anti, broadcast join idea\n- 5 join exercises (dedup, skew awareness, matching semantics)\nDay 5: Window functions (if in course)\n- row_number, dense_rank, lag/lead, partitionBy/orderBy\nDay 6: Graph modelling with DataFrames\n- Build vertices/edges from logs; degree com"}
{"id": "01_study_plan.md#p0#c1", "doc_id": "01_study_plan.md", "page": null, "text": "in course)\n- row_number, dense_rank, lag/lead, partitionBy/orderBy\nDay 6: Graph modelling with DataFrames\n- Build vertices/edges from logs; degree computations\n- Do one motif/triangle via joins\nDay 7: GraphFrames basics\n- Construct GraphFrame; caching\n- Understand algorithm outputs (columns) and parameters\nDay 8: Graph algorithms\n- PageRank, BFS, ConnectedComponents, ShortestPaths, TriangleCount\n- For each: what it returns and a minimal code template\nDay 9: Recommendation & similarity\n- Jaccard/co-occurrence; PPR intuition and pseudo-code\nDay 10: Mock exam\n- 30-min MCQ + 30-min short answers + 60-min coding\n- Review mistakes and update memory sheet\n## 14-day plan\n- Same ordering but add 4 buffer days:\n - extra coding drills, revisiting weak topics, and one additional mock exam.\n## Daily method (闭卷最有效)\n1) 20 min: rewrite key templates from memory (no notes)\n2) 60-90 min: solve 2-3 problem"}
{"id": "01_study_plan.md#p0#c2", "doc_id": "01_study_plan.md", "page": null, "text": " topics, and one additional mock exam.\n## Daily method (闭卷最有效)\n1) 20 min: rewrite key templates from memory (no notes)\n2) 60-90 min: solve 2-3 problems\n3) 20 min: check solutions + write \"common mistakes\" bullets"}
{"id": "02_exam_strategy_closed_book.md#p0#c0", "doc_id": "02_exam_strategy_closed_book.md", "page": null, "text": "# Closed-book Exam Strategy (闭卷应对)\n## 1) How to answer short questions (简答题)\nUse this template:\n- Definition (1 sentence)\n- Why it matters / when it happens (1–2 sentences)\n- Example (mini Spark/graph example)\n- Pitfalls (1–2 bullet points)\nExamples of common short-answer prompts:\n- What is a shuffle? Why is it expensive? How to reduce it?\n- DataFrame vs RDD: differences and use cases\n- Broadcast join: when to use, limitations\n- What does PageRank measure? What are its parameters?\n## 2) Coding questions (编程题)\n### Recommended closed-book structure\n1) Create SparkSession\n2) Load data + schema\n3) Transformations (select/filter/withColumn)\n4) Join / aggregation\n5) Output / show\n### GraphFrames coding structure\n1) Build vertices/edges DataFrames\n2) GraphFrame(v, e)\n3) Run algorithm (bfs/pagerank/cc)\n4) Interpret + show\n## 3) MCQ (选择题)\nHigh-frequency MCQ areas:\n- API names (select vs withColum"}
{"id": "02_exam_strategy_closed_book.md#p0#c1", "doc_id": "02_exam_strategy_closed_book.md", "page": null, "text": "\n2) GraphFrame(v, e)\n3) Run algorithm (bfs/pagerank/cc)\n4) Interpret + show\n## 3) MCQ (选择题)\nHigh-frequency MCQ areas:\n- API names (select vs withColumn)\n- join types semantics\n- caching/persist\n- GraphFrames required column names\n## 4) “If stuck” fallback during exam\n- Write the skeleton first (imports + session + dataframe pipeline)\n- Then fill details (columns, conditions)\n- For graphs: always define vertices(id), edges(src,dst)"}
{"id": "03_glossary.md#p0#c0", "doc_id": "03_glossary.md", "page": null, "text": "# Glossary (Key terms)\n- **Transformation**: Lazy operation that defines a new RDD/DF; executed only when an action is called.\n- **Action**: Triggers computation (collect, count, show, write, ...).\n- **Shuffle**: Data movement across partitions, typically caused by wide transformations (join, groupBy, repartition).\n- **Partition**: A chunk of distributed data processed in parallel.\n- **Broadcast join**: Send small table to all executors to avoid large shuffle.\n- **Catalyst**: Spark SQL optimizer that rewrites logical plans.\n- **Graph**: (V, E) with vertices and edges; often represented by two tables.\n- **GraphFrame**: Graph abstraction over Spark DataFrames (vertices DF + edges DF).\n- **PageRank**: Importance score based on random walk with teleport.\n- **PPR**: Personalized PageRank biased to a seed node with restart.\n- **BFS**: Breadth-first search; finds paths under constraints.\n- **Co"}
{"id": "03_glossary.md#p0#c1", "doc_id": "03_glossary.md", "page": null, "text": " teleport.\n- **PPR**: Personalized PageRank biased to a seed node with restart.\n- **BFS**: Breadth-first search; finds paths under constraints.\n- **Connected components**: Groups of vertices connected in an undirected sense.\n- **Motif finding**: Pattern matching of subgraphs using a declarative syntax."}
{"id": "04_modes_and_outputs.md#p0#c0", "doc_id": "04_modes_and_outputs.md", "page": null, "text": "# Modes & Structured Output Contract\nWe support 4 modes in responses:\n## 1) explain\n- Explain concept and show minimal code skeleton if relevant.\n## 2) quiz\n- Generate questions (MCQ / short answer) with answers and explanations.\n## 3) practice\n- Provide a problem statement + hints, but do NOT reveal full solution immediately.\n## 4) grade\n- User provides an answer/code. Return:\n - score rubric\n - mistakes\n - corrected version\n - key points to review\n## Output must always include\n- `mode`\n- `answer`\n- `key_points` with `importance` (1–5)\n- `sources` (citations)\n- `retrieval` stats (top score, threshold, hit/miss)\n- if retrieval miss: `likely_topics` suggestions"}
{"id": "05_fallback_policy.md#p0#c0", "doc_id": "05_fallback_policy.md", "page": null, "text": "# Fallback behavior when retrieval is weak\nIf the top retrieval similarity is below a threshold, treat the question as \"not found in corpus\".\nThen respond with:\n1) A transparent note: \"I cannot find strong evidence in the provided materials.\"\n2) A *best guess* of where the question fits in the outline (topic suggestions).\n3) Ask 1 clarification question OR suggest what material to add.\nTopic suggestion logic:\n- Match keywords in the question to `topic_map.json` keyword lists\n- Return top 3-5 candidate topics with short reasoning"}
{"id": "topic_map.json#p0#c0", "doc_id": "topic_map.json", "page": null, "text": "[\n {\n \"id\": \"A1\",\n \"title\": \"Spark architecture & execution\",\n \"keywords\": [\n \"driver\",\n \"executor\",\n \"job\",\n \"stage\",\n \"task\",\n \"dag\",\n \"lazy\",\n \"shuffle\",\n \"partition\",\n \"repartition\",\n \"coalesce\"\n ],\n \"related_files\": [\n \"notes/02_spark_core_rdd.md\",\n \"notes/03_spark_dataframes.md\",\n \"meta/00_topic_map.md\"\n ]\n },\n {\n \"id\": \"B3\",\n \"title\": \"DataFrame joins\",\n \"keywords\": [\n \"join\",\n \"inner\",\n \"left\",\n \"right\",\n \"full\",\n \"broadcast\",\n \"semi\",\n \"anti\",\n \"skew\"\n ],\n \"related_files\": [\n \"notes/03_spark_dataframes.md\",\n \"code/joins_aggregations.py\"\n ]\n },\n {\n \"id\": \"D2\",\n \"title\": \"GraphFrames algorithms (PageRank, BFS, CC, ...)\",\n \"keywords\": [\n \"graphframe\",\n \"pagerank\",\n \"bfs\",\n \"connectedcomponents\",\n \"triangle\",\n \"shortestpaths\",\n \"motif\"\n ],\n \"related_files\": [\n \"notes/05_graph_algorithms_pagerank_bfs_cc.md\",\n \"notes/06_graphframes_api_motifs.md\",\n \"code/graphframes_pagerank.py\",\n \"co"}
{"id": "topic_map.json#p0#c1", "doc_id": "topic_map.json", "page": null, "text": "\n ],\n \"related_files\": [\n \"notes/05_graph_algorithms_pagerank_bfs_cc.md\",\n \"notes/06_graphframes_api_motifs.md\",\n \"code/graphframes_pagerank.py\",\n \"code/graphframes_bfs.py\"\n ]\n },\n {\n \"id\": \"E2\",\n \"title\": \"Personalized PageRank (PPR)\",\n \"keywords\": [\n \"ppr\",\n \"personalized pagerank\",\n \"random walk\",\n \"restart\",\n \"seed\"\n ],\n \"related_files\": [\n \"notes/07_reco_ppr_music_graph.md\",\n \"code/ppr_recommendation.py\"\n ]\n }\n]"}
{"id": "01_exam_syllabus.md#p0#c0", "doc_id": "01_exam_syllabus.md", "page": null, "text": "# Exam Syllabus (Spark DataFrames + Graphs)\n> This is a consolidated syllabus built from the provided course materials.\n## A. Spark fundamentals\n- Big Data context (6Vs), Spark ecosystem.\n- Spark architecture: driver, executors, cluster manager; basic execution model.\n- RDD: transformations vs actions, lineage/DAG, fault tolerance.\n## B. Spark SQL & DataFrames (PySpark)\n- SparkSession and reading files (CSV/JSON/Parquet).\n- Schema: inferSchema vs explicit schema; `printSchema`.\n- Core operations:\n - `select`, `withColumn`, `filter/where`, `orderBy`, `drop`, `distinct`.\n - Aggregations: `groupBy().agg(...)`, `count`, `collect_list`.\n - Joins: inner/left/right/full; cross join; join keys.\n - Exploding arrays (`explode`), splitting strings (`split`), string funcs.\n- UDFs: when to use, performance caveats.\n## C. Graph theory basics (needed for algorithms)\n- Directed vs undirected graphs, sim"}
{"id": "01_exam_syllabus.md#p0#c1", "doc_id": "01_exam_syllabus.md", "page": null, "text": "plit`), string funcs.\n- UDFs: when to use, performance caveats.\n## C. Graph theory basics (needed for algorithms)\n- Directed vs undirected graphs, simple vs multigraph vs pseudograph.\n- Degree, in-degree/out-degree, adjacency, paths.\n- Types of networks: random, small-world, scale-free.\n## D. Graph algorithms (concept + implementation patterns)\n- Centrality: degree, closeness, betweenness, eigenvector.\n- PageRank (standard + personalized): definition, damping, convergence.\n- Connected components (undirected) and strongly connected components (directed).\n- Triangle counting and clustering coefficient.\n- BFS/shortest paths (unweighted vs weighted intuition).\n## E. GraphFrames (Spark package)\n- Property graph model: vertices DataFrame + edges DataFrame.\n- Graph algorithms: BFS, connected components, SCC, LPA, PageRank, shortestPaths, triangleCount.\n- Graph queries/pattern matching with `fin"}
{"id": "01_exam_syllabus.md#p0#c2", "doc_id": "01_exam_syllabus.md", "page": null, "text": "DataFrame.\n- Graph algorithms: BFS, connected components, SCC, LPA, PageRank, shortestPaths, triangleCount.\n- Graph queries/pattern matching with `find()` (motifs).\n- Advanced: `aggregateMessages`, Pregel-like API.\n## F. Recommendation mini-case (TME)\n- Construct a heterogeneous graph (users, artists, songs) with weighted edges.\n- Personalized PageRank (PPR) to rank songs for a user.\n## What to be able to do in the exam\n- Explain the concept (what/why), then write short correct PySpark/GraphFrames code.\n- Given a small graph, do 1–2 PageRank/PPR iterations by hand.\n- Interpret outputs (e.g., components, pagerank weights, BFS paths)."}
{"id": "02_spark_core_rdd.md#p0#c0", "doc_id": "02_spark_core_rdd.md", "page": null, "text": "# Spark Core & RDD Essentials\n## Spark architecture (mental model)\n- **Driver**: runs your main program, builds the DAG, schedules jobs.\n- **Executors**: run tasks on worker nodes and store cached data.\n- **SparkSession / SparkContext**: entry point to Spark. The `master` configuration chooses local vs cluster.\n## RDD vs DataFrame (why both exist)\n- **RDD**: distributed collection without schema (low-level, functional transformations + actions).\n- **DataFrame**: distributed table with schema, optimized by **Catalyst** (query optimizer).\n## RDD operations\n### Transformations (lazy)\n- `map`, `flatMap`, `filter`, `union`, `distinct`...\n- Pair-RDD ops: `reduceByKey`, `aggregateByKey`, `join`, `groupByKey`...\n### Actions (trigger computation)\n- `collect` (only if small), `take`, `count`, `reduce`, `saveAsTextFile`...\n## Execution: DAG, Jobs, Stages\n- Transformations build a **lineage graph** "}
{"id": "02_spark_core_rdd.md#p0#c1", "doc_id": "02_spark_core_rdd.md", "page": null, "text": "`collect` (only if small), `take`, `count`, `reduce`, `saveAsTextFile`...\n## Execution: DAG, Jobs, Stages\n- Transformations build a **lineage graph** (DAG).\n- An **action** triggers a job; Spark splits it into stages separated by shuffles.\n## Common exam traps\n- `collect()` on large data can crash the driver.\n- `groupByKey()` often worse than `reduceByKey()` due to shuffle volume.\n- Not understanding laziness → “why didn’t my code run?”"}
{"id": "03_spark_dataframes.md#p0#c0", "doc_id": "03_spark_dataframes.md", "page": null, "text": "# Spark DataFrames Cheat Notes\n## Create a SparkSession\n```python\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"app\").getOrCreate()\n```\n## Read data\n```python\ndf = spark.read.option(\"header\", True).csv(\"path/to/file.csv\")\n```\n## Inspect & debug\n- `df.printSchema()`\n- `df.show(n)`\n- `df.columns`, `df.count()`\n## Core transformations\n### Projection / selection\n```python\nfrom pyspark.sql.functions import col\n(df.select(col(\"id\"), col(\"name\"))\n .filter(col(\"age\") > 30)\n .orderBy(col(\"age\").desc()))\n```\n### Joins\n```python\nres = a.join(b, on=\"id\", how=\"inner\")\n```\nJoin types: `inner`, `left`, `right`, `full`, `left_semi`, `left_anti`.\n### GroupBy / aggregation\n```python\nfrom pyspark.sql.functions import count, avg\n(df.groupBy(\"userId\")\n .agg(count(\"movieId\").alias(\"n\"), avg(\"rating\").alias(\"mean\")))\n```\n### Useful SQL functions (typical)\n- `lower`, `split`, `explo"}
{"id": "03_spark_dataframes.md#p0#c1", "doc_id": "03_spark_dataframes.md", "page": null, "text": "f.groupBy(\"userId\")\n .agg(count(\"movieId\").alias(\"n\"), avg(\"rating\").alias(\"mean\")))\n```\n### Useful SQL functions (typical)\n- `lower`, `split`, `explode`, `max`, `collect_list`, `when`...\n## User-defined functions (UDF)\nPrefer built-in SQL functions when possible.\n```python\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import IntegerType\n@udf(IntegerType())\ndef f(x):\n return 0 if x is None else x\nout = df.withColumn(\"x2\", f(col(\"x\")))\n```\n## Practice checklist\n- Can you: read CSV, clean columns, explode arrays, join, groupBy, build features?"}
{"id": "04_graph_theory_basics.md#p0#c0", "doc_id": "04_graph_theory_basics.md", "page": null, "text": "# Graph Theory Basics (what you must know)\n## Graph types\n- **Directed** vs **undirected**\n- **Simple** vs **multigraph** vs **pseudograph** (multiple edges, loops)\n- **Labeled graphs**: nodes/edges with attributes\n## Core notions\n- **Degree** (undirected), **in-degree/out-degree** (directed)\n- **Adjacency**: two nodes connected by an edge (direction matters)\n- **Path**: sequence of consecutive edges; in directed graphs, follows direction\n## Typical network families\n- Random\n- Small-world (short path lengths)\n- Scale-free (power-law degrees)\n## Graph tasks taxonomy\n- **Exploration / path finding** (BFS, shortest paths)\n- **Centrality / importance** (degree, closeness, betweenness, eigenvector/PageRank)\n- **Community detection** (connected components, label propagation, clustering coefficient)"}
{"id": "05_graph_algorithms_math.md#p0#c0", "doc_id": "05_graph_algorithms_math.md", "page": null, "text": "# Graph Algorithms & Math You Should Be Comfortable With\n## Centralities (conceptual)\n- Degree centrality: number of neighbors\n- Closeness: inverse of sum of shortest-path distances to others\n- Betweenness: fraction of shortest paths that pass through a node\n- Eigenvector centrality: importance influenced by important neighbors\n## PageRank (core formula)\nLet `M` be the transition matrix (column-stochastic for outgoing links).\nIterative update:\n- Standard: `PR_{k+1} = d * M * PR_k + (1-d) * v`\n- `d` (damping) often ~0.85\n- `v` is the personalization vector (uniform for standard PR, or focused on a set of nodes for PPR).\nKey points:\n- Large systems → use **iterative** methods (power iteration), not direct solvers.\n- Handle dangling nodes (no out links) carefully (redistribute mass).\n## BFS / Shortest paths\n- Unweighted shortest path: BFS layers (distance +1)\n- Weighted shortest path: Dijks"}
{"id": "05_graph_algorithms_math.md#p0#c1", "doc_id": "05_graph_algorithms_math.md", "page": null, "text": "out links) carefully (redistribute mass).\n## BFS / Shortest paths\n- Unweighted shortest path: BFS layers (distance +1)\n- Weighted shortest path: Dijkstra (but distributed variants approximate or use iterated relaxations)\n## Triangle counting (idea)\n- Count triangles by checking neighbor pairs.\n- In distributed settings, use ordering constraints (e.g., `v < u < w`) so each triangle counted once.\n## Components\n- Connected components (undirected): maximal set where every pair is connected by a path.\n- Strongly connected components (directed): mutual reachability (path both ways)."}
{"id": "06_graphframes_api.md#p0#c0", "doc_id": "06_graphframes_api.md", "page": null, "text": "# GraphFrames (Spark) — API & Mental Model\n## Why GraphFrames\n- Graphs are hard for pure data-parallel models (irregular structure, shuffles per iteration).\n- GraphFrames gives a **DataFrame API** for graph queries + algorithms.\n## Data model\n- `vertices` DataFrame: **one vertex per row**, must have `id` column.\n- `edges` DataFrame: **one edge per row**, must have `src`, `dst` columns.\n- `triplets`: joined view (src vertex + edge + dst vertex).\n## Common API calls\n- Graph queries / patterns (motifs): `g.find(\"(a)-[e]->(b); (b)-[e2]->(c)\")`\n- Degrees: `g.inDegrees`, `g.outDegrees`, `g.degrees`\n- BFS: `g.bfs(fromExpr, toExpr, edgeFilter=None, maxPathLength=10)`\n## Algorithms (often asked)\n- `pageRank(...)`, `connectedComponents()`, `stronglyConnectedComponents(maxIter)`,\n `shortestPaths(landmarks)`, `triangleCount()`, `labelPropagation(maxIter)`\n## Building your own algorithm\n- `aggregateM"}
{"id": "06_graphframes_api.md#p0#c1", "doc_id": "06_graphframes_api.md", "page": null, "text": "onnectedComponents(maxIter)`,\n `shortestPaths(landmarks)`, `triangleCount()`, `labelPropagation(maxIter)`\n## Building your own algorithm\n- `aggregateMessages(...)` (message passing)\n- DataFrame-based `Pregel` API (`graphframes.lib.Pregel`)\n## Motif constraints\n- You cannot have completely anonymous edges like `()-[]->()` with no named element.\n- Negated edges cannot be named."}
{"id": "07_graph_dataframe_algorithms.md#p0#c0", "doc_id": "07_graph_dataframe_algorithms.md", "page": null, "text": "# Implementing Graph Algorithms with DataFrames (Map/Reduce view)\nThis section focuses on the *DataFrame style* (join + groupBy) implementations.\n## PageRank as DataFrame operations (sketch)\n1. Keep `edges(src, dst, weight)` where weight distributes rank (`1/outDegree` or custom).\n2. At each iteration:\n - join edges with ranks on `src`\n - contribution = `d * rank(src) * weight`\n - aggregate by `dst` with `sum(contribution)`\n - new_rank(dst) = aggregated + (1-d) * personalization(dst)\n## Triangle counting (sketch)\n- Generate neighbor pairs per node (potential triangles) and then join with real edges to validate.\n- Use ordering constraints to avoid duplicates.\n## BFS style (iterative relaxations)\n- Maintain `distances(id, d)` and an `active` set.\n- Expand from active nodes by joining with edges and updating tentative distances.\n- Stop when there is no update."}
{"id": "08_recommendation_ppr_workflow.md#p0#c0", "doc_id": "08_recommendation_ppr_workflow.md", "page": null, "text": "# Recommendation with Personalized PageRank (PPR) — Workflow\n## Graph construction idea (music example)\nYou build a heterogeneous graph:\n- user → song edges (weights sum to 1 per user)\n- user → artist edges (weights sum to 1 per user)\n- artist → song edges (weights sum to 1 per artist)\n- song ↔ song edges (similarity links; graph of songs is undirected)\nThen you run **Personalized PageRank** from a given user node to rank songs.\n## PPR iteration (concept)\nLet `x` be the current importance vector.\n- Initialize: `x[user] = (1-d)`, others 0.\n- Update rule (high-level): propagate mass along outgoing edges with damping `d`,\n and inject teleport mass `(1-d)` back to the personalization source.\n## Output\n- Sort songs by score and filter out songs already listened by the user.\n- Return top-N recommendations.\n## Exam angle\n- Be able to explain: (1) why weights must be normalized, (2) how personal"}
{"id": "08_recommendation_ppr_workflow.md#p0#c1", "doc_id": "08_recommendation_ppr_workflow.md", "page": null, "text": "already listened by the user.\n- Return top-N recommendations.\n## Exam angle\n- Be able to explain: (1) why weights must be normalized, (2) how personalization changes PR."}
{"id": "09_exam_question_patterns.md#p0#c0", "doc_id": "09_exam_question_patterns.md", "page": null, "text": "# Typical Exam Question Patterns (and what they test)\n## Spark DataFrames\n- “Write a query that…”: projection/filter/orderBy/groupBy/joins.\n- “Explain why collect() is dangerous.”\n- “Difference between RDD and DataFrame, and why Catalyst helps.”\n## Graph algorithms\n- Define degree/in-degree/out-degree/path.\n- Distinguish: connected vs strongly connected components.\n- Explain PageRank and damping; write the iterative equation.\n- BFS vs Dijkstra; unweighted vs weighted shortest paths.\n- Triangle counting: why ordering avoids double counting.\n## GraphFrames\n- Build vertices/edges DataFrames and create a GraphFrame.\n- Use `find()` motif to express a pattern.\n- Run `bfs` or `pageRank` and interpret results."}
{"id": "10_revision_strategy.md#p0#c0", "doc_id": "10_revision_strategy.md", "page": null, "text": "# Revision Strategy (how to practice efficiently)\n## 1) Fast recall loop (15–20 min)\n- Re-write from memory:\n - PageRank equation\n - join + groupBy patterns\n - GraphFrame schema requirements (id/src/dst)\n## 2) Hands-on drills (60–90 min)\n- DataFrames: load CSV → clean → join → aggregate → export.\n- GraphFrames: build tiny graph → BFS → connected components → PageRank.\n## 3) Error-focused review\nCommon issues to force yourself to debug:\n- type mismatches (string vs int)\n- missing columns (`id`, `src`, `dst`)\n- duplicates / multiple edges\n- exploding arrays leads to row explosion\n## 4) If the exam has coding\n- Keep a personal “starter template” (SparkSession, imports, helpers)\n- Time yourself on 2–3 end-to-end problems."}
{"id": "11_dataframe_optimization_and_performance.md#p0#c0", "doc_id": "11_dataframe_optimization_and_performance.md", "page": null, "text": "# DataFrames: Optimization & Performance (Exam-ready)\n## Execution model\n- Transformations build a **logical plan**; actions trigger execution.\n- Spark SQL uses **Catalyst optimizer** to rewrite the plan; physical plan chooses join strategy, partitioning, etc.\n## Partitions & shuffles\n- Wide transformations (join, groupBy, distinct, repartition) often trigger **shuffle**.\n- Use `explain()` to check if there is `Exchange` (shuffle) in the physical plan.\n## Join strategies (common questions)\n- Broadcast hash join: one side small -> `broadcast(df_small)`.\n- Sort-merge join: large-large, requires sorting/shuffle.\n- Tips:\n - Filter early, select needed columns.\n - Avoid skew (salting, AQE if available).\n## Cache/Persist\n- `df.cache()` stores computed partitions in memory (lazy until action).\n- Use when the same expensive subplan is reused.\n## UDF vs built-in functions\n- Prefer built-in SQL fu"}
{"id": "11_dataframe_optimization_and_performance.md#p0#c1", "doc_id": "11_dataframe_optimization_and_performance.md", "page": null, "text": "puted partitions in memory (lazy until action).\n- Use when the same expensive subplan is reused.\n## UDF vs built-in functions\n- Prefer built-in SQL functions: optimized + codegen.\n- UDF can be slower; also limits optimization.\n## Window functions\n- Pattern: ranking, moving average, top-k per group.\n- Typical: `Window.partitionBy(...).orderBy(...)`.\n## Debugging performance\n- `df.explain(True)`\n- Spark UI: stages, tasks, shuffle read/write, skew."}
{"id": "12_graph_algorithms_exam_cheatsheet.md#p0#c0", "doc_id": "12_graph_algorithms_exam_cheatsheet.md", "page": null, "text": "# Graph Algorithms Cheat Sheet (Definitions + What to memorize)\n## Core definitions\n- Graph: G=(V,E), directed/undirected, weighted/unweighted.\n- Degree: in-degree/out-degree for directed.\n- Path, walk, cycle, simple path.\n## BFS / shortest path\n- BFS gives shortest path in **unweighted** graphs.\n- Complexity: O(|V|+|E|) with adjacency list.\n## Connected components\n- Undirected CC: nodes connected by some path.\n- Algorithm idea (distributed): label propagation / union-find style iterations.\n## PageRank\n- Random surfer model: PR(v) = (1-d)/|V| + d * sum_{u->v} PR(u)/outdeg(u)\n- d (damping) typically 0.85\n- Convergence: iterate until delta < eps or maxIter.\n## Triangle count\n- Triangle: 3-cycle (undirected).\n- DataFrame approach: self-joins on edges with ordering constraints.\n## Motifs (GraphFrames)\n- Use pattern matching to find subgraphs, e.g. triangles: `(a)-[e1]->(b); (b)-[e2]->(c); (c"}
{"id": "12_graph_algorithms_exam_cheatsheet.md#p0#c1", "doc_id": "12_graph_algorithms_exam_cheatsheet.md", "page": null, "text": "n edges with ordering constraints.\n## Motifs (GraphFrames)\n- Use pattern matching to find subgraphs, e.g. triangles: `(a)-[e1]->(b); (b)-[e2]->(c); (c)-[e3]->(a)`\n## Personalized PageRank (PPR)\n- Similar to PageRank but teleport to a **seed set** S.\n- Use for recommendations: “items close to my seed nodes”."}
{"id": "13_common_exam_pitfalls.md#p0#c0", "doc_id": "13_common_exam_pitfalls.md", "page": null, "text": "# Common Exam Pitfalls (Spark + Graph)\n## Spark\n- Confusing **transformations** vs **actions**.\n- Forgetting that `cache()` is lazy.\n- Misunderstanding join output columns (duplicate names, need `alias`).\n- Not handling nulls after joins.\n- Using `collect()` on large datasets.\n- Skew in groupBy/join -> timeouts.\n## DataFrames vs RDD\n- DataFrames are optimized by Catalyst; prefer them for SQL-like tasks.\n- RDD is lower-level; you manage schema and optimization manually.\n## Graph\n- Directed vs undirected: PageRank uses direction; CC usually undirected.\n- BFS/shortest path: BFS works only for unweighted; for weighted use Dijkstra.\n- Triangle counting: must avoid double-counting by ordering constraints.\n## GraphFrames\n- Vertex column name must be `id` (default) unless configured.\n- Edge columns must include `src`, `dst`.\n- Motif `find()` returns columns with nested structs; you often need `."}
{"id": "13_common_exam_pitfalls.md#p0#c1", "doc_id": "13_common_exam_pitfalls.md", "page": null, "text": "be `id` (default) unless configured.\n- Edge columns must include `src`, `dst`.\n- Motif `find()` returns columns with nested structs; you often need `.selectExpr`."}
{"id": "coding_tasks.json#p0#c0", "doc_id": "coding_tasks.json", "page": null, "text": "{\n \"meta\": {\n \"version\": \"1.0\",\n \"format\": \"coding\"\n },\n \"tasks\": [\n {\n \"id\": \"code_001\",\n \"topic\": \"dataframe_join_agg\",\n \"difficulty\": \"easy\",\n \"prompt\": \"Given two DataFrames (users, purchases), compute total spend per user and return top 10 users.\",\n \"requirements\": [\n \"Use join + groupBy + agg\",\n \"Handle users with no purchases (left join, fill null with 0)\",\n \"Sort by total descending\"\n ],\n \"starter_file\": \"code/03_joins_and_aggs.py\"\n },\n {\n \"id\": \"code_002\",\n \"topic\": \"graphframes_pagerank\",\n \"difficulty\": \"medium\",\n \"prompt\": \"Build a GraphFrame from vertices/edges and compute PageRank; return top-k vertices by rank.\",\n \"requirements\": [\n \"Vertices must have column id\",\n \"Edges must have src,dst\",\n \"Use maxIter or tol\"\n ],\n \"starter_file\": \"code/06_graphframes_pagerank.py\"\n },\n {\n \"id\": \"code_003\",\n \"topic\": \"triangle_count\",\n \"difficulty\": \"hard\",\n \"prompt\": \"Count triangles usi"}
{"id": "coding_tasks.json#p0#c1", "doc_id": "coding_tasks.json", "page": null, "text": "_file\": \"code/06_graphframes_pagerank.py\"\n },\n {\n \"id\": \"code_003\",\n \"topic\": \"triangle_count\",\n \"difficulty\": \"hard\",\n \"prompt\": \"Count triangles using only DataFrame joins (no GraphFrames).\",\n \"requirements\": [\n \"Avoid double counting\",\n \"Explain your join conditions\",\n \"Return triangle list + count\"\n ],\n \"starter_file\": \"code/09_triangle_count_dataframe_only.py\"\n }\n ]\n}"}
{"id": "math_exercises.json#p0#c0", "doc_id": "math_exercises.json", "page": null, "text": "{\n \"meta\": {\n \"version\": \"1.0\",\n \"format\": \"math\"\n },\n \"exercises\": [\n {\n \"id\": \"math_001\",\n \"topic\": \"pagerank\",\n \"difficulty\": \"medium\",\n \"question\": \"Derive the PageRank update equation and explain the role of the teleport term.\",\n \"expected\": [\n \"PR(v) = (1-d)/N + d * sum_{u->v} PR(u)/outdeg(u)\",\n \"Teleport ensures ergodicity, handles sinks/spider traps\"\n ]\n },\n {\n \"id\": \"math_002\",\n \"topic\": \"complexity\",\n \"difficulty\": \"easy\",\n \"question\": \"Give time complexity of BFS on adjacency list and why it is O(V+E).\",\n \"expected\": [\n \"Each node visited once, each edge explored at most once\"\n ]\n }\n ]\n}"}
{"id": "mcq.json#p0#c0", "doc_id": "mcq.json", "page": null, "text": "{\n \"meta\": {\n \"version\": \"1.0\",\n \"format\": \"mcq\",\n \"topics\": [\n \"spark\",\n \"dataframes\",\n \"graphs\",\n \"graphframes\"\n ]\n },\n \"questions\": [\n {\n \"id\": \"mcq_001\",\n \"topic\": \"spark_basics\",\n \"difficulty\": \"easy\",\n \"question\": \"Which statement is true about Spark transformations?\",\n \"choices\": [\n \"They execute immediately when called\",\n \"They are lazy and build a logical plan\",\n \"They always trigger a shuffle\",\n \"They can only be applied to RDDs\"\n ],\n \"answer_index\": 1,\n \"explanation\": \"Transformations are lazy; actions trigger execution.\"\n },\n {\n \"id\": \"mcq_002\",\n \"topic\": \"dataframes\",\n \"difficulty\": \"easy\",\n \"question\": \"What is the main benefit of DataFrames over RDDs?\",\n \"choices\": [\n \"They allow arbitrary Python objects\",\n \"They are optimized by Spark SQL (Catalyst) and can use efficient execution\",\n \"They never spill to disk\",\n \"They do not require a SparkSession\"\n ],\n \"answer_index\": 1,"}
{"id": "mcq.json#p0#c1", "doc_id": "mcq.json", "page": null, "text": "d by Spark SQL (Catalyst) and can use efficient execution\",\n \"They never spill to disk\",\n \"They do not require a SparkSession\"\n ],\n \"answer_index\": 1,\n \"explanation\": \"DataFrames are optimized and can leverage code generation and SQL planning.\"\n },\n {\n \"id\": \"mcq_003\",\n \"topic\": \"joins\",\n \"difficulty\": \"medium\",\n \"question\": \"When is a broadcast hash join typically beneficial?\",\n \"choices\": [\n \"When both tables are huge\",\n \"When one table is small enough to fit in memory on each executor\",\n \"When keys are not sortable\",\n \"When you want to force a shuffle\"\n ],\n \"answer_index\": 1,\n \"explanation\": \"Broadcasting the small table avoids shuffling the large table.\"\n },\n {\n \"id\": \"mcq_004\",\n \"topic\": \"graphframes\",\n \"difficulty\": \"easy\",\n \"question\": \"In GraphFrames, what are the required column names for edges by default?\",\n \"choices\": [\n \"from,to\",\n \"src,dst\",\n \"u,v\",\n \"start,end\"\n ],\n \"answer"}
{"id": "mcq.json#p0#c2", "doc_id": "mcq.json", "page": null, "text": "\": \"In GraphFrames, what are the required column names for edges by default?\",\n \"choices\": [\n \"from,to\",\n \"src,dst\",\n \"u,v\",\n \"start,end\"\n ],\n \"answer_index\": 1,\n \"explanation\": \"GraphFrames expects edge columns `src` and `dst` by default.\"\n },\n {\n \"id\": \"mcq_005\",\n \"topic\": \"pagerank\",\n \"difficulty\": \"medium\",\n \"question\": \"In PageRank, the damping factor d mainly controls:\",\n \"choices\": [\n \"The maximum path length\",\n \"The probability of following links vs teleporting\",\n \"Whether the graph must be undirected\",\n \"The number of connected components\"\n ],\n \"answer_index\": 1,\n \"explanation\": \"d is the probability of following outgoing links; (1-d) is teleport.\"\n }\n ]\n}"}
{"id": "short_answer.json#p0#c0", "doc_id": "short_answer.json", "page": null, "text": "{\n \"meta\": {\n \"version\": \"1.0\",\n \"format\": \"short_answer\"\n },\n \"questions\": [\n {\n \"id\": \"sa_001\",\n \"topic\": \"spark_execution\",\n \"difficulty\": \"easy\",\n \"question\": \"Explain the difference between a transformation and an action in Spark. Give two examples of each.\",\n \"expected_points\": [\n \"Transformations are lazy and build a DAG (e.g., select, filter, withColumn, join)\",\n \"Actions trigger execution (e.g., count, collect, write)\",\n \"Spark optimizes before execution\"\n ]\n },\n {\n \"id\": \"sa_002\",\n \"topic\": \"shuffle\",\n \"difficulty\": \"medium\",\n \"question\": \"What is a shuffle in Spark? List operations that commonly trigger shuffles and how to reduce them.\",\n \"expected_points\": [\n \"Data redistribution across partitions\",\n \"Triggered by groupBy, join, distinct, orderBy, repartition\",\n \"Reduce by filtering early, broadcasting small table, pre-partitioning, using correct join keys\"\n ]\n },\n {\n \"id\": \""}
{"id": "short_answer.json#p0#c1", "doc_id": "short_answer.json", "page": null, "text": " distinct, orderBy, repartition\",\n \"Reduce by filtering early, broadcasting small table, pre-partitioning, using correct join keys\"\n ]\n },\n {\n \"id\": \"sa_003\",\n \"topic\": \"graphframes_motifs\",\n \"difficulty\": \"medium\",\n \"question\": \"What is motif finding in GraphFrames? Provide an example pattern for triangle detection.\",\n \"expected_points\": [\n \"Subgraph pattern matching over edges\",\n \"Triangle example: (a)-[e1]->(b); (b)-[e2]->(c); (c)-[e3]->(a)\",\n \"Need constraints to avoid duplicates\"\n ]\n }\n ]\n}"}
{"id": "construction-graphe-DataFrame-TME.pdf#p1#c0", "doc_id": "construction-graphe-DataFrame-TME.pdf", "page": 1, "text": "TME: Construction du graphe utilisateurs, artistes,\nchansons\nGraphe de chansons: Non dirigé"}
{"id": "construction-graphe-DataFrame-TME.pdf#p2#c0", "doc_id": "construction-graphe-DataFrame-TME.pdf", "page": 2, "text": "TME: Calcul de recommandation avec PPR\nImportance de la page i a l’itération courante: x[i]\nImportance de la page i a l’itération précédente: xant[i]"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p1#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 1, "text": "Apache Spark DataFrames Source: Cloud Computinget BigData --FC Machine Learning et IA (B. Amann)"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p2#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 2, "text": "Data gathering and processing\n2"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p3#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 3, "text": "Big Data, Data Sciences and AI\n3"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p4#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 4, "text": "Big Data Stack\n4"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p5#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 5, "text": "The Challenges of Big Data: The 6 Vs\n5\nWeneedto providesolutions (languages, models, systems, hardware) thatmeetthesenew needs."}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p6#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 6, "text": "The Challenges of Big Data Systems\n6"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p7#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 7, "text": "Parallel architecture (cluster)\n7"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p8#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 8, "text": "Virtualization (cloud)\n8"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p9#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 9, "text": "Challenges and Choices: The CAP theorem\n9"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p10#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 10, "text": "The CAP theorem\n10"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p11#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 11, "text": "CAP and noSQL : One Size Fits All\n11"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p12#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 12, "text": "Computations: Task and data parallelism\n12\nData parallelismis the simultaneous execution of the same function across the elements of a data set.Task parallelismis the simultaneous execution of multiple and different functions across the same or different data sets."}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p13#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 13, "text": "Apache Spark\n13"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p14#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 14, "text": "Apache Spark and the PySpark API❏Apache Spark ❏open-source project, unified analytics engine for large-scale data processing(combines data and AI)❏written in Scala, provides high-level APIs in Java, Scala, Python, R, SQL❏uses the Hadoop MapReduce distributed computing framework❏stores and works with large volumes of data in memory (memory persistence)❏batch processing ❏Resilient Distributed Dataset (RDD)❏logical collection of data distributed across multiple nodes (data that can be processed in parallel)❏large granule treatment (no partial modification)❏reference data on disk (HDFS, S3, etc. ), data in central memory or other RDD on which it depends❏Operations as DAGs:❏order and relationship between operations❏fault tolerance by re-execution of a processing chain\n14"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p15#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 15, "text": "Apache Spark and the PySpark API❏Spark ecosystem❏Spark Core❏Spark SQL❏Spark Streaming and Structured Streaming❏Machine Learning Library (MLlib)❏GraphX/GraphFrames (general graph processing)❏PySpark❏a set of Spark APIsin Pythonlanguage❏supports all of Spark’s features: Spark SQL, DataFrames, Structured Streaming, Machine Learning (MLlib) and Spark Core\n15\nSpark: a unified pipeline"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p16#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 16, "text": "Spark Architecture\n16"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p17#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 17, "text": "Cluster overview (Architecture of a Spark Application)https://spark.apache.org/docs/latest/cluster-overview.html\n17\n1.Master connects to a Cluster manager to allocate resources across applications2.Acquires Executors on cluster nodes (run compute tasks, cache data)3.Sends app code to Executors4.Sends tasks to Executors"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p18#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 18, "text": "Spark Context●Tells Spark how to access the cluster●Used subsequently to create other variables●Master parameter for SparkContext:○Determines which cluster to use\n18"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p19#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 19, "text": "RDD (Resilient Distributed Dataset)\n19\n●Primary abstraction of Spark●A distributed collection of data elements without any schema●Can contain any data type (Python, Java, Scala objects, including user defined classes)●Can handle structured and unstructured data easily and effectivelyCreation of RDDs from:●local memory●local/distributed file/set of files●existing database●another RDD: transformationsTwo types of operations on RDDs:●Transformations●Actions"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p20#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 20, "text": "RDD: Distributed Data Abstraction\n20\nWorker Node 1Worker Node 2Worker Node 3Worker Node 4RDD\nDriver Application\nSparkApplicationSparkApplicationSparkApplicationSparkApplication\nRDD: partitioned across the clusterComputations on the RDD are executed in parallel on each partition"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p21#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 21, "text": "RDD Operations: transformations\n21\nRDD 2\nRDD 1\nTransformationTransformationTransformationTransformation\n❏create anewRDD from an existing one❏lazily evaluated(not executed immediately): create a lineage of transformations❏only computed when an action requires a result to be returned to the driver program.❏Advantages:❏optimize the required calculations❏Recover from lost data partitions"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p22#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 22, "text": "Common transformations on RDD\n22\n●pyspark.RDD.map: Return a new RDD by applying a function to each element of this RDD.RDD.map(f: Callable[[T], U], preservesPartitioning: bool = False) → pyspark.rdd.RDD[U]…●pyspark.RDD.flatMap: Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results.RDD.flatMap(f: Callable[[T], Iterable[U]], preservesPartitioning: bool = False) → pyspark.rdd.RDD[U]…●pyspark.RDD.filter: Return a new RDD containing only the elements that satisfy a predicate.RDD.filter(f: Callable[[T], bool]) → pyspark.rdd.RDD[T]"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p23#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 23, "text": "Examples of transformations: map, filter# import of the SparkContext libraryfrom pyspark import SparkContext# environment initialization (local with 4 cores)sc = SparkContext(\"local[4]\", \"first app\")# definition of an RDD from a python array (2 partitions)rdd =sc.parallelize([1,2,3,4],2)# definition of the Python inc functiondef inc(x): return x+1 # map transformation with the inc functionrdd_map = rdd.map(inc)# definition of the Python test functiondef test(x): return x!=4# filter transformation with the test functionrdd_filter = rdd.filter(test) 23\nrdd{1, 2, 3, 4}\nmap_rdd{2, 3, 4, 5}filter_rdd{1, 2, 3}"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p24#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 24, "text": "Transformations on Pair RDD●RDD[(K, V)]: can be both iterated and indexed●pyspark.RDD.aggregateByKey: Aggregate the values of each key, using given combine functions and a neutral “zero value”.aggregateByKey(zeroValue: U, seqFunc: Callable[[U, V], U], combFunc: Callable[[U, U], U], numPartitions: Optional[int] = None, partitionFunc: Callable[[K], int] = <function portable_hash>) → pyspark.rdd.RDD[Tuple[K, U]]\n●RDD.reduceByKey:Merge the values for each key using an associative and commutativereduce function.reduceByKey(func: Callable[[V, V], V], numPartitions: Optional[int] = None, partitionFunc: Callable[[K], int] = <function portable_hash>)→ pyspark.rdd.RDD[Tuple[K, V]]\n●RDD.join: Return an RDD containing all pairs of elements with matching keys in self and other. Each pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in self and (k, v2) is in other.RDD.join(o"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p24#c1", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 24, "text": "g keys in self and other. Each pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in self and (k, v2) is in other.RDD.join(other: pyspark.rdd.RDD[Tuple[K, U]], numPartitions: Optional[int] = None) → pyspark.rdd.RDD[Tuple[K, Tuple[V, U]]] 24"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p25#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 25, "text": "Examples of transformations: reduceByKey, joinPython Functions–def inc(x) : return x+1Notation :–lambda x: (x+1)\n25\nrdd{a, b, a, c}\nmap_rdd{(a,1), (b,1), (a,1), (c,1)}\nred_rdd{(a,2), (b,1), (c,1)}\nrdd=sc.parallelize([‘a’,’b’,’a’,’c’], 2)def ind(x): return (x,1) map_rdd = rdd.map(ind)# ormap_rdd = rdd.map(lambda x: (x,1))def add(x,y): return x+yred_rdd = map_rdd.reduceByKey(add)# orred_rdd = map_rdd.reduceByKey(lambda x,y: x+y)rdd2 =sc.parallelize([(\"a\", 1), (\"b\", 4)])join_rdd = red_rdd.join(rdd2)\nrdd2{(a,1), (b,4)}\nred_rdd{(a,(1,2)), (b,(1,4))}"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p26#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 26, "text": "List of Transformations (1)\n26"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p27#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 27, "text": "List of Transformations(2)\n27"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p28#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 28, "text": "RDD Operations: Actions\n28\nRDD\nSpark Driver Application\nActionActionActionAction\n❏Causes full execution of transformations, return a value to the driver program or write the data to an external storage❏Each transformed RDD may be recomputed each time an action is run on it (unless persisted in memory/disk/replicated across cluster)"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p29#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 29, "text": "Common actions on RDD\n29\n●pyspark.RDD.collect: Return a list that contains all the elements in a RDD (should only be used if the resulting array is expected to be small, as all the data is loaded into the driver’s memory).RDD.collect() → List[T]…●pyspark.RDD.take: Take the first num elements of a RDD.RDD.take(num: int) → List[T]…●pyspark.RDD.reduce:Combine all elements of a RDD to a single result of the same type, using an associative and commutative reduce function.RDD.reduce(f: Callable[[T, T], T]) → T…●pyspark.RDD.aggregate:Aggregate the elements of each partition, and then the results for all the partitions, using a given combine functions and a neutral “zero value.”RDD.aggregate(zeroValue: U, seqOp: Callable[[U, T], U], combOp: Callable[[U, U], U]) → U"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p30#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 30, "text": "Example of Actions# definition of the Python add functiondef add(x,y): return x+y# application of the reduce action with the commutative and associative add function# (the result is a Python value)red = map_rdd.reduce(add)map_rdd.count()filter_rdd.take(2)# writes into mydirectoryon the HDFS home directoryfilter_rdd.saveAsTextFile(“mydirectory”)# collect(): retrieve all the elements of the dataset (from all nodes) to the driver node(data must fit into memory) # result: Python Arraylocal_array= filter_rdd.collect()\n30\nrdd{1, 2, 3, 4}\nmap_rdd{2, 3, 4, 5}filter_rdd{1, 2, 3}\nreduce() = 14count() = 4take(2) = {1,2}saveAsTextFilecollect()=[1,2,3]"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p31#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 31, "text": "List of Actions (1)\n31"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p32#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 32, "text": "List of Actions (2)\n32"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p33#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 33, "text": "RDD Lineage Graph\n33\nExecution workflow:●managed as a directed acyclic graph (DAG), executed when actions are executed\nWhen an action is requested:●Spark walks the RDD dependency graph backwards●Creates aJob(sequence of transformations on data)"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p34#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 34, "text": "RDD: conclusionImmutable: ●each transformation creates a new RDD (advantage: optimization) Resilient (fault-tolerant):●can be recovered ( recreated ) in any point of the execution time from the “lineage” of each RDD (the sequence of operations that produced it)Lazy evaluated: ●RDDs are materialized only when an action is performed (an entire chain of the graph is executed)Two types of operations:\n34"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p35#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 35, "text": "History of Spark APIs\n35"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p36#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 36, "text": "Spark Datasets and DataframesDataframe(Schema RDD):●Dataframe= Dataset[Row]●Row = tuple with attributes●Implemented in Scala, Java, Python, RDataset:●RDD + SQL●Implemented in Scala and Java\nSpark SQL●Package pyspark.sql●Dataframeand Datasets●« SQL » operators●Optimization : Catalyst optimizer\n36"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p37#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 37, "text": "Spark DataFrames\n37"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p38#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 38, "text": "Creating a session\n38"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p39#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 39, "text": "DataFrame: creation/file reading\n39"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p40#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 40, "text": "DataFrame: print schema\n40"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p41#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 41, "text": "DataFrame: projection, selection\n41"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p42#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 42, "text": "DataFrame: orderBy\n42"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p43#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 43, "text": "DataFrame: column rename\n43"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p44#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 44, "text": "DataFrame: explode\n44"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p45#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 45, "text": "DataFrames: join, cross join (cartesian product)\n45"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p46#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 46, "text": "DataFrames: group by/aggregate functions\n46"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p47#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 47, "text": "Aggregate function: collect_list\n47"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p48#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 48, "text": "SQL functions: lower, max\n48"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p49#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 49, "text": "SQL functions: split\n49"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p50#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 50, "text": "DataFrames: filtering, new columns\n50"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p51#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 51, "text": "DataFrames: user functions\n51"}
{"id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf#p52#c0", "doc_id": "Lecture1-SparkDataFrames-eng-2025-2026.pdf", "page": 52, "text": "●Advantages:○cluster computing platform, fast and fault-tolerant large-scale computations○Rich API for fast advanced analytics: ‘MAP’ and ‘reduce’ operations, ML algorithms, graphs, SQL queries, streaming data, etc.○Processing of high volumes of structured and unstructured data ○Speed due to low latency RAM(in-memory data processing ability)○Dynamic: parallel execution of applications○Multi-Language engine: Java, Scala, Python, R, SQL○Very flexible: data from various data sources can be used (usage with HDFS)●Disadvantages:○Dependence on external storage systems○Large number of Small files○Less number of algorithms in MLlib.○High memory consumption and increased hardware costs:○Unsuitable for for multi-user environments○Non automatic optimization process\n52\nSpark: conclusion"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p1#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 1, "text": "Graph algorithms in\nDataFrames"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p2#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 2, "text": "GRAPH TYPES\n•directed : social network, bibliographic\ncitations, hypertext web, semantic web,\nrecommendation graph, evolution graph, etc.\n•undirected : road network, collaboration\nnetwork, co-occurrence graphs,\n•labeled :\n•nodes: name, age, content\n•arcs: friendships, cost, duration, .."}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p3#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 3, "text": "GRAPH TYPES\n•simple : a single edge between each pair of\nnodes\n•multigraph : several edges between each pair of\nnodes\n•pseudograph : multiple edges and loops"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p4#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 4, "text": "Undirected graph: Adjacent nodes\n¢There is an edge that connects them"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p5#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 5, "text": "Undirected graph: Degree of a node\nVertex F has degree 3 (number of incident edges)"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p6#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 6, "text": "Path in an undirected graph\nFinite sequence of consecutive edges, connecting B to C (in both directions)"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p7#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 7, "text": "Adjacent nodes in a directed graph\nNode B is adjacent to node F because there is a path from B to F\n(F is not adjacent to B)"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p8#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 8, "text": "Degree in/out in a directed graph\nIn degree of F is 1, out degree of F is 2"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p9#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 9, "text": "Types of networks\nThree representative types:\n•Random: all nodes have the same probability of\nbeing connected to another node\n•Small world: very short paths between all\nnodes (e.g. social networks)\n•Scale-free: power law for node degree\ndistribution, some very connected nodes and\nmany poorly connected nodes (e.g. WWW)"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p10#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 10, "text": "Types of graph algorithms\nExploration/\nPathfinding\nFind optimal paths or\nassess availability and quality\nroutes\nCentrality/\nImportance\nFind the importance of\ndifferent nodes of the network\nDetection of\ncommunities\nFind clusters or partitions\nTYPES OF GRAPH ALGORITHMS"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p11#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 11, "text": "CENTRALITY-BASED ALGORITHMS"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p12#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 12, "text": "Degree centrality\n•The simplest among centrality-based algorithms\n•Degree: Number of edges of the node (number of neighbors)\n•Application: find popular nodes ( eg popular people in a social network)\n•All nodes are treated the same àdoes not take into account the importance of nodes"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p13#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 13, "text": "Closeness Centrality\n•Important nodes have the shortestdistances to all othernodes\n•Measuresthe averagefarness(inverse distance) froma nodeto all othernodes.\n•Application: detect nodes capable of spreading information in a subgraph (egpeople in\ngood position to control and acquire information and resources within an organization)\n!\"= 1\n∑&' \"((*, ,)"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p14#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 14, "text": "Betweenness centrality\n•Important nodesare on manyof the shortestpaths betweenothernodes(central nodes for\nthe information circulating in the graph)\n•Captures a node'srolein allowing information to passfromone part of the network to the\nother\n•Application: detect the degree of influence of a node on the flow of information in a graph\n( e.ga person capable of connecting people from different companies, passing ideas\nbetween communities, etc. )\n!\"=$\n%&\"&'\n((*, ,, -)\n((*, -)"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p15#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 15, "text": "Eigenvector centrality\n•scores relative to all nodes in the network\n•scores influenced more by connections to high-scoring nodes than by connections\nto low-scoring nodes.\n•Important nodes: surrounded by important neighbors\n•The centrality score of a node is the sum of the centrality scores of its neighbors\n(recursive definition)\n!is a constant usedfor normalization(the largesteigenvalueof the\nadjacencymatrix)"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p16#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 16, "text": "PageRank: Importance of Web Pages"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p17#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 17, "text": "Eigenvector centrality\n•variants: PageRank and Katz centrality score\nRewriting in matrix form:\nA = Adjacencymatrix (Auv= 1 si u ∈N(v))\ncmax= eigenvectorof the largesteigenvalue\"max"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p18#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 18, "text": "PageRank: principle\nHyperlinks = recommendations\nPrinciple:\n• Pages with a lot of recommendations are more important\n• Also important: who gives the recommendation\n• Principles:\n•being recommended by Yahoo! is better than by X\n•the recommendation counts less if Yahoo! recommends\nlots of pages\n→importance of a page depends on the number and on the quality\n(importance) of the one who recommends it\nPageRank: principles\nn1 n2n1 recommendsn2"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p19#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 19, "text": "Simplified PageRank\nRecommendation given by n1 to n2 :\nThe importance of n2 is the sum of the recommendations received:\nni= pages that recommend n2\nSimplified PageRank\nn1 n2n1 recommendsn2\nPR(n1) = importance of n1\n|out(n1)| = out degree of n1"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p20#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 20, "text": "PR(A) = PR(N) /3 + PR(Y)\nPR(Y) = PR(N) /3 + PR(G)/2\nPR(N) = PR(G)/2\nPR(G) = PR(A) + PR(N)/3\nExample"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p21#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 21, "text": "Linear system resolution\n• 4 equations with 4 unknowns\n• test the existence and uniqueness of the\nsolution\nPR(A) = PR(N) /3 + PR(Y)\nPR(Y) = PR(N) /3 + PR(G)/2\nPR(N) = PR(G)/2\nPR(G) = PR(A) + PR(N)/3\n→ add the constraint PR(A)+PR(Y)+PR(N)+PR(G) = 1 to ensure uniqueness\nObservation:\n• large linear system, many pages without outgoing links\nÞdirect calculation methods (e.g. Gauss method) are more expensive than\niterative methods\nComputation of PR values"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p22#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 22, "text": "We consider n pages, for each page i, we note:\n•out(j) : the set of pages i referenced by j\n•M( wij ) : the adjacency matrix associated with the Web graph\n•wij: fraction of the importance of j which is given to i ( wij= 1/|out(j)| ,\nif j has outgoing links, wij=0 otherwise)\nline i = importance fractions received by i\ncolumn j = distribution of importance of j\n(for pages j with outgoing links, the sum of the elements on the columns is 1)\n•Pri : importance of page i\n•PR(PR1, PR2,..., PRn ) : vector of unknowns (importance)\nMatrix representation"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p23#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 23, "text": "Iterative computation algorithm\nqInput: a graph withn nodes\nqOutput: the PageRank vector\nqInitialization: PR0= [1/n, ..., 1/n]\nqRecomputePR(k)at eachiterationk:\nqStop the computation (convergence) :\nqThe PR vectorobtainedat convergence satisfies the condition:"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p24#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 24, "text": "PR(A) = PR(N) /3 + PR(Y)\nPR(Y) = PR(N) /3 + PR(G)/2\nPR(N) = PR(G)/2\nPR(G) = PR(A) + PR(N)/3\nExample"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p25#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 25, "text": "PR(A) = PR(N) /3 + PR(Y)\nPR(Y) = PR(N) /3 + PR(G)/2\nPR(N) = PR(G)/2\nPR(G) = PR(A) + PR(N)/3\nExample"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p26#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 26, "text": "K (K-1)\nExample: computation at each\niteration"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p27#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 27, "text": "Example: update for each page\nUpdate of Pri:"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p28#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 28, "text": "PR(A) = 1 /3 + 1\nPR(Y) = 1/3 + 1/2\nPR(N) = 1/2\nPR(G) = 1+ 1/3\nWeight of the arcs for the example: importance fraction ( w ij * PR j )\nExample: first iteration"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p29#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 29, "text": "PR(A) = 0.5 /3 + 0.85\nPR(Y) = 0.5/3 + 1.33/2\nPR(N) = 1.33/2\nPR(G) = 1.33+ 0.5/3\nWeight of the arcs for the example: importance fraction ( w ij * PR j )\nExample: second iteration"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p30#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 30, "text": "Complete iterative algorithm\nqInput: a graph withn nodes\nqOutput: the PageRank vector\nqInitialization: PR0= [1/n, ..., 1/n]\nqVectoradded at eachiteration: V = [1/n, ..., 1/n]\nqRecomputePR(k)at eachiterationk:\nor itsequivalent:\nstops when:\nqThe decay factor d (usually0.85) isusedto ensurethe uniquenessof the\nPR vector and the convergence of the iterative computation\nqPersonalization(specific importance for a set E of nodes)\nV = [v1, ..., vn] (vi= 1/E if i ∈E, vi= 0 otherwise)"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p31#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 31, "text": "Example: Complete iterative\nalgorithm"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p32#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 32, "text": "PageRank in MapReduce\nPR(A) = d *PR(N) * w AN + d *PR(Y)* w AY + (1-d) *p A\nPR(G)\nBreakdown of the calculation at each iteration k into two steps:\nMAP: calculation of values d*PR j\nk * wij\nREDUCE: Sum of previous values and addition of\npersonalization values\nMAP MAP REDUCE\nPageRank in DataFrames\nStep 1\nStep 2\nStep 1\nStep 1 Step 2"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p33#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 33, "text": "Example\nn2\nn1\nn4\nn3"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p34#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 34, "text": "PageRank in MapReduce\nn1[n3] n2[n1,n3] n3[n4] n4[n1, n2]\nn3 n1 n3 n4 n1\nMap\nn2 n4\nn2\nReduce\nn1[n3] n2[n1,n3] n3[n4] n4[n1, n2]\nn4 n1 n2 n3\nNext iteration….\nPageRank in DataFrames\nStep 2\nStep 1"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p35#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 35, "text": "Example: MAP\nn2\nn1\nn4\nn3\nw13\nw21\nw23\nw42\nw41\nw34\nN.PAGERANK"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p36#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 36, "text": "Example: MAP\nn2\nn1\nn4\nn3\nw13\nw21\nw23\nw42\nw41\nw34\npm←d∗wnm∗N.PAGERANK\n= d * w13 *\n= d * w41 *\n= d * w42 *\n= d * w34 *\n= d * w21 *\n= d * w23 *\nStep 1"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p37#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 37, "text": "Example: REDUCE\nn2\nn1\nn4\nn3\nw13\nw21\nw23\nw42\nw41\nw34\nM.PAGERANK ← s+(1-d)*MP\n+ + (1-d) * n1.PERS()=\n+ (1-d) * n2.PERS()=\n+ (1-d) * n4.PERS()=\n+( ) + (1-d) * n1.PERS=\nStep 2"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p38#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 38, "text": "Example: personalized PR\nn2\nn1\nn4\nn3\nid p\n21\nw21\nw23\nw13 w41\nw42\nw34\nSD w\n13 w13\n21 w21\n23 w23\n34 w34\n41 w41\n42 w42\ngraph\nP\nd = 0.85\nPR = importance to compute\nP = personalization vector (wrt. n2)\nid i\n1 i1\n2 i2\n3 i3\n4 i4\nPR"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p39#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 39, "text": "Example: First iteration\nn2\nn1\nn4\nn3\nid i\n2 i2_0\nw21\nw23\nw13 w41\nw42\nw34\nSD w\n13 w13\n21 w21\n23 w23\n34 w34\n41 w41\n42 w42\ngraph\nPR=P\n= i2_0=1\n= i2_0*d*w21\n= i2_0*d*w23\nD next-i\n1 i2_0 * d *w21\n3 i2_0 * d *w23\nd∗∑ij\n0\n∗wji\nPRi\n1\n=d∗∑ ij\n0\n∗wji+( 1−d)∗Pi\nFirst\niteration\nid i\n1 i2_0 * d * w21\n3 i2_0 * d * w23\n2 (1-d) * 1"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p40#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 40, "text": "Example: second iteration\nn2\nn1\nn4\nn3\nw21\nw23\nw13 w41\nw42\nw34\nSD w\n13 w13\n21 w21\n23 w23\n34 w34\n41 w41\n42 w42\ngraph\ni2_1\n= i2_1*d*w21=(1-d)*d*w21\n= i2_1*d*w23= (1-d)*d*w23\nDnext-i\n1 i2_1* d *w21\n3 i2_1*d*w23+i1_1*d*\nw13\n4 i3_1* d *w34\nd∗∑ij\n1\n∗wji PRi\n2\n=d∗∑ij\n1\n∗wji+( 1−d)∗Pi\nSecond\niteration\nid i\n1 i1_1\n3 i3_1\n2 i2_1\ni1_1\ni3_1\n= i3_1* d * w34=(1*d*w23)*d*w34\n= i1_1 *d * w13=(1*d*w21)*d*w13\nid i\n1 i2_1* d *w21\n3 i2_1*d*w23+i1_1*d*w13\n4 i3_1* d *w34\n2(1-d)*1\ni2_2= 1-d\ni1_2= i2_1* d *w21\ni3_2= i2_1*d*w23\n+i1_1*d*w13\ni4_2= i3_1* d *w34"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p41#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 41, "text": "Example: third iteration\nn2\nn1\nn4\nn3\nw21\nw23\nw13 w41\nw42\nw34\nSD w\n13 w13\n21 w21\n23 w23\n34 w34\n41 w41\n42 w42\ngraph\n= i2_2*d*w21\n= i2_2*d*w23\nDnext-i\n1 i2_2* d w21+i4_2d*w41\n3 i2_2* d*w23+i1_2*d*w13\n4 i3_2* d *w34\n2i4_2* d *w42\nd∗∑ij\n2\n∗wji PRi\n3\n=d∗∑ij\n2\n∗wji+( 1−d)∗Pi\nThird\niteration\n= i3_2* d * w34\n= i1_2*d * w13\nid i\n1 i2_2* d w21+i4_2d*w4\n3 i2_2* d*w23+i1_2*d*w13\n4 i3_2* d *w34\n2 i4_2* d *w42+(1-d)*1\nid i\n1 i1_2\n3 i3_2\n2 i2_2\n4 i4_2\n= i4_2*d * w41\n= i4_2*d * w42\ni2_3= i4_2*d*w42+(1-d)*1\ni1_3\ni3_3\ni4_3"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p42#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 42, "text": "Algorithms for community\ndetection"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p43#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 43, "text": "Clustering coefficient\nCC(\nCC(\nCC(\nCC(\n) = 1/1\n) = 2/3\n) = 1/1\n) = 2/3\nClustering coefficient for an undirected graph G=(V ,E):\nl measure of grouping nodes in a graph, used in small world networks\n(social networks), community detection\nl cc(v) = fraction of neighbors of v which are themselves neighbors\nClustering coefficient : the probability that a node's neighbors are connected to each other"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p44#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 44, "text": "Clustering coefficient\nShows the density of connectivity around a node\nvs.\nCC( ) = 0.1CC() = 0.5\nApplications:\n•study the community structure of the Facebook social graph (find dense\nneighborhoods of users in the global sparse graph)\n•explore the thematic structure of the Web: detect page communities/common\ntopics on the basis of reciprocal links."}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p45#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 45, "text": "Counting triangles\n•Clustering coefficient: Represents the number of triangles in the graph\n3 triangles out of the 6 possible triples"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p46#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 46, "text": "How to count triangles?\nSequential algorithm (undirected graph):\nNbTriangles← 0\nforeach node v:\nforeach pair u,w in Γ(v)\nif (u,w) is an edge\nNbTriangles += 1/2\nreturn (NbTriangles / 3)\nComplexity of the algorithm:\nEach triangle is counted 3 times (once per node)"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p47#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 47, "text": "Example: computation of triangles\nNbTriangles= 0\nv\nFrom node v"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p48#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 48, "text": "Example\nNbTriangles = 1/2\nu\nw\nv\nFrom node v"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p49#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 49, "text": "Example\nNbTriangles = 1\nw\nv\nu\nFrom node v"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p50#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 50, "text": "Example\nNbTriangles = 1.5\nv\nt\nw\nFrom node v"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p51#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 51, "text": "Example\nNbTriangles = 2\nv\nw\nt\nFrom node v"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p52#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 52, "text": "MapReduce algorithm\nMap 1 : Input: {(v,u)| }\nforeach emit {(v,u)}\nExample :\nReduce 1 : Input: {(v,u) | }\nforeach(u,w):\nemit {((u,w), v)} //all possible edges in the graph\nExample : ,.....\nu∈Γ(v)\nu∈Γ(v)\nu,w∈Γ(v)\n(( ((( ( ))))))\nMap 2: emit { ((u,w), $)| }\nReduce 2: Input : {(u,w)| v1,v2,...vk,$?}\nforeach (u,w) if $ is part of the input, then :\nNbTriangles[vi] +=1/2\nw∈Γ(u)\n(( ) )→ NbTriangles( ) +1/2\n(\n$\n) →∅\nu∈Γ(v)\n( ( ( ( ( )) )) ))( )(\nStep 1\nStep 2\nStep 3\nStep 4\nDataFrame"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p53#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 53, "text": "Algorithm adaptation\nWe generate all the paths to check in parallel, the execution time is\n• => for nodes with many neighbors (millions) the\ncorresponding reducer tasks can be very slow\nImprovement :\norder the nodes by their degree (for those which have the same\ndegree by their identifier)\ncount each triangle only once, starting from the minimum node\ncomplexity: (m = number of arcs in the graph)\nO(m3/2)\nmaxv∈V(∑dv\n2\n)"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p54#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 54, "text": "Improved algorithm\nSequential algorithm:\nnbTriangles← 0\nforeach v in V\nforeach u,w in Adjacency(v)\nif u > v && w > u\nif (u,w) in E\nnbTriangles++\nreturn nbTriangles"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p55#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 55, "text": "Parallel algorithm\nMap 1 : Input: {(v,u)| }\nif u > v then emit {(v,u)}\nExample :\nReduce 1: Input : {(v,u) | }\nforeach (u,w) : //u,w: neighbors of v (v< u et v< w)\nif w > u then emit {((u,w), v)}\nExample :\nu∈Γ(v)\nu∈S⊂Γ(v)\nu,w∈S << <\n() ( )(( () ) )\n(( ((( ( ))))))\nMap 2: emit { ((u,w), $)| }, u < w\nReduce 2: Input : {(u,w)| v1,v2,...vk,$?}\nforeach (u,w) if $ is part of the input, then: NbTriangles[vi] ++\nExample :\nw∈Γ(u)\n(( ) )→ NbTriangles( )++\n(\n$\n) → ∅\n)\nDataFrame\nStep 1\nStep 2\nStep 3\nStep 4"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p56#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 56, "text": "Computation with DataFrames\nMap 1: Input : {(v,u)| }\nif u > v then emit {(v,u)}\nExample :\nu∈Γ(v)\n<< <\n() ( )(( () ) )\nv u\nv u\nStep 1"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p57#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 57, "text": "Computation with DataFrames\nReduce 1: Input : {(v,u) | }\nforeach (u,w) : //u,w: neighbors of v (v< u et v< w)\nif w > u then emit {((u,w), v)}\nExamples:\nu∈S⊂Γ(v)\nu,w∈S\n<< <(( ((( ( )))))) )\nv u v list_u v list_u\n( ) )(()\nv pair\n)(\n()\n( )\nStep 2"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p58#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 58, "text": "Computation with DataFrames\n<< <\nMap 2: emit { ((u,w), $)| }, u < ww∈Γ(u)\nv u edge\n( )\n( )\n( )\n( )\n( )\nStep 3"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p59#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 59, "text": "Computation with DataFrames\n<< <\nReduce 2: Input : {(u,w)| v1,v2,...vk,$?}\nforeach (u,w) if $ is part of the input, then: NbTriangles[vi] ++\nedge\n( )\n( )\n( )\n( )\n( )\nv pair\n)(\n()\n( )edge=pair\nv pair\n)(\n( )\nv nbTriangles\n2\nStep 4"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p60#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 60, "text": "Strongly connected components:\ndirected graph\n¢There is a path between each pair of vertices (maximum strongly connected subgraph)"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p61#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 61, "text": "Strongly connected components:\ndirected graph\n¢ Applications:\n¢find sets of companies in which each member directly and/or indirectly owns shares of all other\nmembers\n¢calculate the connectivity of different network configurations when measuring routing performance\nin multi-hop wireless networks.\n¢T wo strongly connected components"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p62#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 62, "text": "Connected Components:\nUndirected Graph\n¢Set of vertices connected in pairs by a path (maximum connected subgraph) which are not\nconnected to other nodes in the graph\n¢If there is a path between nodes, they must belong to the same component\n¢ Applications:\n¢Analysis of citation networks\n¢determine the degree of connectivity of a network (see if connectivity maintained if one\nremoves a central node or an authority node)"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p63#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 63, "text": "Algorithms for path finding and graph\nexploration"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p64#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 64, "text": "BFS and DFS\nBFS(Neo4J and Spark )\n•Choose a source node (1)\n•Explore all neighbors as far as possible in a chosen order (2, 3, 4)\n•Visit neighbors' neighbors (5,6,7, 8....)\n•Stop: the entire graph has been visited or a specific criterion is satisfied\nDFS\n•Choose a source node (1)\n•Choose a neighbor, visit one of your neighbors, etc. , as far away as possible\n•Choose another neighbor and start again: 1, 2, 3, 4, 5, 6, 7...\n•Stop: the entire graph has been visited or a specific criterion is satisfied"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p65#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 65, "text": "Path in a directed graph\nSequence of consecutive arcs connecting node A to node H (the path follows\nthe direction of the arcs)"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p66#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 66, "text": "Shortest paths\n¢Find the shortest\npath between a source node\nand a destination node\nàdepends on the weight\nof the edges\nApplications:\n•Kevin Bacon number (number of degrees of separation between actors and Kevin Bacon,\nbased on the films they starred in)\n•LinkedIn uses the shortest path algorithm\n•Google Maps : Find Routes Between Places"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p67#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 67, "text": "Unweighted graph\n¢All edges have the same weight (=1) àshortest path with minimum number of vertices"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p68#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 68, "text": "Unweighted graph\n¢There are other, longer paths (5)"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p69#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 69, "text": "Weighted graph\n¢Shortest path àdepends on the sum of edge weights (Dijkstra algorithm)"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p70#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 70, "text": "Weighted graph\n¢Shortest path cost: 1+1+2+3+1 = 8"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p71#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 71, "text": "Weighted graph\n¢Path cost: 12+19+5 = 36"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p72#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 72, "text": "Shortest paths"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p73#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 73, "text": "Computation\nFind the length of the shortest path from a given node s to other nodes\nCentralized: Dijkstra's algorithm\nMapReduce: Parallel BFS from initial node s\nIntuition (for an unweighted graph):\nl For all neighbors p of s:\nDISTANCE(p) = 1\nl For all nodes n reachable from a set of nodes M\nDISTANCE(n) = 1+ min(DISTANCE(m))m∈M"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p74#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 74, "text": "BFS: weighted graph\nn0\nn3\nn2\nn1\nn4\nn5\nn6\nn70\ninf\ninf\ninf\ninf\ninf\ninf\ninf10\n22 11\n3\n4\n1\n3\n2\n5\n1\n2\nFirst iteration:\nl Active node: n0\nn0\nn3\nn2\nn1\nn4\nn5\nn6\nn70\ninf\ninf\ninf\ninf10\n22 11\n3\n4\n1\n3\n2\n5\n1\n2\nMap Reduce\n3\n2\n10Step 1 Step 2"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p75#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 75, "text": "BFS: weighted graph\nn0\nn3\nn2\nn1\nn4\nn5\nn6\nn70\n10\n2\n3\ninf\ninf\ninf\ninf10\n22 10+1\n2+1\n3\n4+10\n2+1\n3\n3+2\n2+5\n1\n2\nSecond iteration:\nl Active nodes: n1, n2, n3\nn0\nn3\nn2\nn1\nn4\nn5\nn6\nn70\n5\n10\n22 11\n3\n4\n1\n3\n2\n5\n1\n2\nMap Reduce\n3\n2\n10\n14\n3\n3\nStep 1 Step 2"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p76#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 76, "text": "BFS: weighted graph\nn0\nn3\nn2\nn1\nn4\nn5\nn6\nn70\n10\n2\n3\n5\n3\n3\n1410\n22 1\n2+1\n3+3\n4\n1\n3\n2\n5\n1+3\n2+3\nThird iteration:\nl Active nodes: n4, n5, n6, n7\nn0\nn3\nn2\nn1\nn4\nn5\nn6\nn70\n4\n10\n22 11\n3\n4\n1\n3\n2\n5\n1\n2\nMap Reduce\n3\n2\n6\n14\n3\n3\nStep 1 Step 2"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p77#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 77, "text": "BFS: weighted graph\nn0\nn3\nn2\nn1\nn4\nn5\nn6\nn70\n6\n2\n3\n4\n3\n3\n1410\n22 6+1\n1\n3\n6+4\n1\n3\n2\n5\n1\n2\nFourth iteration:\nl Active nodes: n4, n1\nn0\nn3\nn2\nn1\nn4\nn5\nn6\nn70\n4\n10\n22 11\n3\n4\n1\n3\n2\n5\n1\n2\nMap Reduce\n3\n2\n6\n10\n3\n3\nStep 1 Step 2"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p78#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 78, "text": "BFS: weighted graph\nn0\nn3\nn2\nn1\nn4\nn5\nn6\nn70\n6\n2\n3\n4\n3\n3\n1010\n22 1\n1\n3\n4\n1\n3\n2\n5\n1\n2\nFifth iteration:\nl Active node: n7\nn0\nn3\nn2\nn1\nn4\nn5\nn6\nn70\n4\n10\n22 11\n3\n4\n1\n3\n2\n5\n1\n2\nMap Reduce\n3\n2\n6\n10\n3\n3\nIn summary:\nl A node becomes active when its distance has been updated\nl Stop the computation when there are no more active nodes\nStep 1 Step 2"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p79#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 79, "text": "BFS: Exemple avec DataFrame\n(Map)\nn0\nn3\nn2\nn1\nn4\nn5\nn6\nn70\n10\n2\n3\ninf\ninf\ninf\ninf10\n22 10+1\n2+1\n3\n4+10\n2+1\n3\n3+2\n2+5\n1\n2\nSecond iteration\nidA d\nn1 10\nn2 2\nn3 3\nactive\nSD w\nn0 n1 10\nn0 n3 3\nn0 n2 2\nn1 n7 4\nn1 n2 1\nn2 n4 5\nn2 n5 1\nn2 n6 1\nn3 n4 2\nn5 n4 1\nn5 n6 2\nn6 n1 3\ngraph\nid d\nn2 10+1\nn4 2+5\nn4 3+2\nn5 2+1\nn6 2+1\nn7 4+10\nnew_distances\nStep 1"}
{"id": "Lecture2-graph-DataFrames-eng-25.pdf#p80#c0", "doc_id": "Lecture2-graph-DataFrames-eng-25.pdf", "page": 80, "text": "BFS: Example with DataFrame\n(Reduce)\n3\nSecond iteration\nn0\nn3\nn2\nn1\nn4\nn5\nn6\nn70\n5\n10\n22 1 1\n3\n4\n1\n3\n2\n5\n1\n2\nReduce\n3\n2\n10\n14\n3\n3\nid d\nn2 10+1\nn4 2+5\nn4 3+2\nn5 2+1\nn6 2+1\nn7 4+10\nnew_distances\nid d\nn0 0\nn1 10\nn2 2\nn3 3\nn4 inf\nn5 inf\nn6 inf\nn7 inf\nprev_distances\nid d\nn0 0\nn1 10\nn2 2\nn3 3\nn4 5\nn5 3\nn6 3\nn7 14\ndistances\nStep 2"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p1#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 1, "text": "GraphFrames"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p2#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 2, "text": "Large-scale graph processing\nLimitations of data-parallel models (e.g Map/Reduce, Spark):\n●Graphs have irregular structure\n○Diﬃcult to extract parallelism based on partitioning of the data → Poorly partitioned data: unbalanced\ncomputation loads\n○Power-law degree distributions for large real-world graphs (social networks, www) → Computation and\ndata access patterns have poor locality of memory access\n●The graph is shuﬄed at each iteration (vertex object N (including the OUT adjacency list) passed as a parameter\nto map and reduce methods → Better to send only the new importance value and not the structure of the\ngraph\n●The iterations are controlled outside M/R (termination conditions and programme logic)\n●Little computation required for each vertex\n●Degree of parallelism changes during the execution"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p3#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 3, "text": "Graph-Parallel Abstraction\nProgram graph computations:\n“Think like a vertex” (Malewicz et al. [SIGMOD’ 10])\nPrinciples:\n●each vertex has a small neighborhood to maximize\nparallelism\n●eﬀective graph partitioning to minimize\ncommunication"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p4#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 4, "text": "Graph-Parallel Abstraction\n●A user-deﬁned Vertex-Program runs on each vertex\n●Graph constrains interactions along edges:\n○using messages (e.g. Pregel)\n○through shared state (e.g., GraphLab)\n●Parallelism: run multiple vertex programs simultaneously"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p5#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 5, "text": "Data-Parallel vs. Graph-Parallel Computation\nData-parallel\nGraph-parallel\nData-parallel computations:\n●Record-centric view of data\n●Parallelism: processing independent data on separate sources\nGraph-Parallel computations:\n●Vertex-centric views of graphs\n○Speciﬁc graph partitioning techniques (graph-dependent)\n○Resolve dependencies (along edges) by iterative\ncomputation\n○Restrict the types of computation\n○Communication along edges\n○Exploit graph structure to achieve performance gains of\nseveral orders of magnitude compared with more\ngeneral data-parallel systems."}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p6#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 6, "text": "BSP Model●Bulk synchronous parallel (BSP) : parallel programming model for designing data-parallel algorithms.●Vertex-centric System. ●Principle:○Series of iterations (supersteps)○At each step (iteration) S, local execution for each vertex V: ■invokes a function in parallel■can read messages sent in previous superstep (S-1)■can send messages to be read at next superstep (S+1)■can modify the state of itself and of the outgoing edges ■can modify graph’s topology○Messages:■Message value + destination vertex■Sent along outgoing edges, can be sent to any vertex with known id■Only available at the beginning of a superstep ■Guaranteed to be delivered and not duplicated ■Can be out of order●Pregel:○Graph parallel computing framework based on BSP\n○Proposed by Google, open source implementations : Apache Giraph, Stanford GPS, Apache Hama"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p7#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 7, "text": "Computation model\n●Input: a directed graph\n○Each vertex: an identiﬁer and a modiﬁable value\n○Each directed edge: a source vertex, a target vertex,\na modiﬁable value\n●At each superstep:\ncomputation→communication→barrier synchronization\n●Synchronisation barrier: ○at the end of each superstep○ensures that all messages have been transmitted but not yet delivered ○message delivery: beginning of the next superstep: ensures deadlock-free execution."}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p8#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 8, "text": "T ermination\n●In superstep 0 all vertices are active\n●Only active vertices participate in a superstep\n●Active vertices can vote to halt → become inactive\n●Inactive vertices can be reactivated upon receiving a message → become active\n●Algorithm termination: when all nodes vote to halt (all vertices are inactive)\nand there are no messages in transit\n●Output: set of values output by vertices"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p9#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 9, "text": "Example: PageRank in Pregel\nVertex process executes\nCompute() during each\nsuperstep\nSuperstep 0 (Initialization):\n1/NumVertices() for each vertex\nin msgs\nout msgs"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p10#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 10, "text": "Example: Single Source Shortest Paths in Pregel"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p11#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 11, "text": "Example: Single Source Shortest Paths\nSuperstep 0\nInitialization Superstep 0\nCommunication\nSuperstep 0\nSynchronisation"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p12#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 12, "text": "Example: Single Source Shortest Paths-Superstep 1\nSuperstep 1\nGetMessages\nSuperstep 1\nUpdate Values\nSuperstep 1\nCommunicate\nSuperstep 1\nSynchronisation"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p13#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 13, "text": "Example: Single Source Shortest Paths-Supersteps 2-4\nSuperstep 2 Superstep 3 Superstep 4:\nHalt for all vertices\nResult :\n(1,0.0) (2,12.0) (3,14.0) (4,8.0) (5,9.0) (6,3.0)"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p14#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 14, "text": "Example: ﬁnding maximum value\nDotted Arrows: messages\nGrey nodes: inactive"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p15#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 15, "text": "Advantages of the BSP ModelAdvantages:\n●Vertex-centric approach:\n○Structured way to develop parallel algorithms\n○Users focus on a local action\n○Process each item independently\n●Ease of Reasoning/Predictability of the performance/behavior of parallel algorithms: clear structure of\nBSP (computation, communication, synchronization)\n●Better optimization and resource allocation: predict execution time of parallel algorithms\n●Scalability: an be eﬀectively used on small multi-core processors or large distributed systems.\n●Synchronization barrier:\n○helps in managing workloads that are not uniformly distributed among processors, load\nbalancing\n○reduces the overhead of frequent synchronization\n●Abstraction of the communication layer: focus on algorithm development=>BSP algorithms portable\nacross diﬀerent parallel architectures\n●No deadlocks and data races common in asynchronous systems"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p16#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 16, "text": "Limitations of the BSP Model\n●Ineﬃcient if diﬀerent graph regions converge at diﬀerent speed.\n●Can suﬀer if one task is more expensive than the others\n●Runtime of each phase: determined by the slowest machine\n●Diﬃcult to express the diﬀerent stages of a processing pipeline on graphs (building/modifying\nthe graph, computations on several graphs)"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p17#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 17, "text": "Example: Graph analysis pipelineDiﬃcult to use and program\n●Users have to learn, deploy and manage multiple systems\nLeads to interfaces that are complicated to implement and often complex to use\nIneﬃcient :\n●Generate large amounts of data movement and duplication across the network and ﬁle system\n●Limited re-use of internal data structures from one stage to the next"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p18#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 18, "text": "Problem: Mixed Graph Analysis\nSame data, diﬀerent vues (table or\ngraph), easily change between them"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p19#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 19, "text": "GraphX: Unifying Graphs and Tables\nNew API\nReduces the distinction between\nTables and Graphs\nNew System\nCombines Data-Parallel and\nGraph-Parallel systems\nEnables users:\n●Easily and eﬃciently express the entire graph analysis pipeline.\n●View data both as collections (RDD) and as a graph without data\nmovement/duplication"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p20#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 20, "text": "Example: Graph analysis pipeline with GraphX\nGraphX:\n●processing time for the entire\npipeline is faster than\nSpark+GraphLab"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p21#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 21, "text": "Graph algorithms in GraphX (lines of code)\nPregel and GraphLab algorithms are implemented with GraphX operators in less\nthan 50 lines of code."}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p22#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 22, "text": "GraphX: different views\nTables and Graphs are composable views of the same physical data.\nEach view has its own operators which exploit the semantics of the view to achieve\neﬃcient execution."}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p23#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 23, "text": "Property Graphs●Directed multigraphs with user-deﬁned objects attached to each edge and vertex\n(allow to model several relationships between nodes)\n●Each vertex has a unique 64-bit VertexID key\n●Each edge has the ID of the source vertex and the ID of the destination vertex\n●Two RDDs: VertexRDD[VD] (for vertices) and EdgeRDD[ED] (for edges)\n○VD, ED: types of the objects associated with vertices/edges\nLike RDDs, property graphs are:\n●Immutable: changes to the values or structure of the graph are made by producing\na new graph.\n●Distributed: the graph is partitioned using a set of node partitioning heuristics\n●Fault-tolerant: as with RDD, each partition on the graph can be recreated on\nanother machine for fault tolerance purposes"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p24#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 24, "text": "Property Graph\nEdge Table: EdgeRDD[ED]\nED: String\nVertex Table:\nVertexRDD[VD]\nVD: (String, String)\nGraph((String, String), String)"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p25#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 25, "text": "RDD Operations\nTable operators (RDD) are inherited from Spark"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p26#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 26, "text": "Graph Operations (1)\n/** Summary of the functionality in the property graph */\nclass Graph[VD, ED] {\n // Information about the Graph ===================================================================\n val numEdges: Long\n val numVertices: Long\n val inDegrees: VertexRDD[Int]\n val outDegrees: VertexRDD[Int]\n val degrees: VertexRDD[Int]\n // Views of the graph as collections =============================================================\n val vertices: VertexRDD[VD]\n val edges: EdgeRDD[ED]\n val triplets: RDD[EdgeTriplet[VD, ED]]\n// Transform vertex and edge attributes ==========================================================\n def mapVertices[VD2](map: (VertexId, VD) => VD2): Graph[VD2, ED]\n def mapEdges[ED2](map: Edge[ED] => ED2): Graph[VD, ED2]\n def mapTriplets[ED2](map: EdgeTriplet[VD, ED] => ED2): Graph[VD, ED2]\n// Modify the graph structure ================================================"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p26#c1", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 26, "text": "\n def mapTriplets[ED2](map: EdgeTriplet[VD, ED] => ED2): Graph[VD, ED2]\n// Modify the graph structure ====================================================================\n def reverse: Graph[VD, ED]\n def subgraph(epred: EdgeTriplet[VD,ED] => Boolean = (x => true),\n vpred: (VertexId, VD) => Boolean = ((v, d) => true)): Graph[VD, ED]\n def mask[VD2, ED2](other: Graph[VD2, ED2]): Graph[VD, ED]\n def groupEdges(merge: (ED, ED) => ED): Graph[VD, ED]\n// Change the partitioning heuristic ============================================================\n def partitionBy(partitionStrategy: PartitionStrategy): Graph[VD, ED]"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p27#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 27, "text": "Graph Operations (2)\n// Aggregate information about adjacent triplets =================================================\n def collectNeighborIds(edgeDirection: EdgeDirection): VertexRDD[Array[VertexId]]\n def collectNeighbors(edgeDirection: EdgeDirection): VertexRDD[Array[(VertexId, VD)]]\n def aggregateMessages[Msg: ClassTag](\n sendMsg: EdgeContext[VD, ED, Msg] => Unit,\n mergeMsg: (Msg, Msg) => Msg,\n tripletFields: TripletFields = TripletFields.All)\n : VertexRDD[A]\n // Iterative graph-parallel computation ==========================================================\n def pregel[A](initialMsg: A, maxIterations: Int, activeDirection: EdgeDirection)(\n vprog: (VertexId, VD, A) => VD,\n sendMsg: EdgeTriplet[VD, ED] => Iterator[(VertexId, A)],\n mergeMsg: (A, A) => A)\n : Graph[VD, ED]\n // Basic graph algorithms ========================================================================\n def pageRank(tol"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p27#c1", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 27, "text": "g: (A, A) => A)\n : Graph[VD, ED]\n // Basic graph algorithms ========================================================================\n def pageRank(tol: Double, resetProb: Double = 0.15): Graph[Double, Double]\n def connectedComponents(): Graph[VertexId, ED]\n def triangleCount(): Graph[Int, ED]\n def stronglyConnectedComponents(numIter: Int): Graph[VertexId, ED]\n// Functions for caching graphs ==================================================================\n def persist(newLevel: StorageLevel = StorageLevel.MEMORY_ONLY): Graph[VD, ED]\n def cache(): Graph[VD, ED]\n def unpersistVertices(blocking: Boolean = false): Graph[VD, ED]"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p28#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 28, "text": "Apache Spark’s GraphX Library\n●General purpose graph processing library\n●Built into Spark\n●Optimized for fast distributed computing\n●Library of algorithms: PageRank, Connected Components, etc\nLimitations:\n●No Java and Python APIs\n●Lower-level RDD-based API (vs. DataFrames)\n●Cannot use recent Spark optimizations: Catalyst query optimizer,\nTungsten memory management\n●No support for graph queries/patterns"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p29#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 29, "text": "Graph Algorithms vs. Graph QueriesGraph queries: identify an explicit pattern within the graph\nE.g: common friends of users Alice and Bob,\n all friends of Alice which are not friends of Bob\nGraph algorithms: complex computations, graph traversal\nE.g: most influential people (Page Rank), shortest paths from Alice to Bob"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p30#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 30, "text": "Graph Algorithms vs. Graph Queries\nGraph Algorithm: Single Source Shortest Paths\nval sssp= graph.pregel(Double.PositiveInfinity)(\n (id, dist, newDist) => math.min(dist, newDist), //\nVertex Program\n triplet => { // Send Message\n if (triplet.srcAttr + triplet.attr <\ntriplet.dstAttr) {\n Iterator((triplet.dstId, triplet.srcAttr +\ntriplet.attr))\n } else {\n Iterator.empty\n }\n },\n (a, b) => math.min(a, b) // Merge Message\n)\nGraph Query: Select subgraph based on edges \"e\"\nof type \"follow\" pointing from a younger user\n\"a\" to an older user \"b\".\npaths = g.find(\"(a)-[e]->(b)\")\\\n .filter(\"e.relationship = 'follow'\")\\\n .filter(\"a.age < b.age\")"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p31#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 31, "text": "Separate Systems"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p32#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 32, "text": "Solution: GraphFrames"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p33#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 33, "text": "GraphFrames Spark package●Spark package introduced in 2016\n○Collaboration between Databricks, UC Berkley and MIT\n●DataFrames API for Spark\n○High-level API in Java, Python and Scala.\n○Simpliﬁes interactive queries\n○Beneﬁts from DataFrames optimizations\n○Integrates with the rest of Spark ecosystem\n●GraphFrames are to DataFrames as GraphX are to RDDs\n○Expressive graph queries, pattern matching\n○Query plan optimizers from Spark SQL\n○Graph algorithms\n●Not yet integrated into the Spark architecture"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p34#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 34, "text": "GraphFrames vs GraphX"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p35#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 35, "text": "GraphFrames API\n●Uniﬁes graph algorithms, graph queries and DataFrames\n●Available in Java, Scala and Python\nclass GraphFrame {\ndef vertices: DataFrame\n def edges: DataFrame\ndef ﬁnd(pattern: String): DataFrame\ndef degrees (): DataFrame\ndef pageRank (): GraphFrame\ndef connectedComponents (): GraphFrame\n}"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p36#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 36, "text": "Supported graph algorithms\n●Breadth-ﬁrst search (BFS)\n●Connected components\n○Strongly connected components\n●LPA: label propagation algorithm\n●PageRank and Personalized PageRank\n●Shortest paths\n●Triangle count"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p37#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 37, "text": "Vertices DataFrame\n g.vertices.show()\nVertices DataFrame:\n●1 vertex per Row\n●Id: column with unique ID"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p38#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 38, "text": "Edges DataFrame\ng.edges.show()\nEdges DataFrame:\n●1 edge per Row\n●src, dst: column using IDs from\nvertices.id\nExtra columns store vertex of edge data\n(attributes or properties)"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p39#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 39, "text": "Triplets DataFrame\n●Extends the information in edges with informations on the vertices\n●Join vertices and edges to get (src, edge, dst)\ng.triplets.show()"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p40#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 40, "text": "Computation of vertex degrees\n# Get a DataFrame with columns \"id\" and \"inDegree\" (in-degree)\nvertexInDegrees = g.inDegrees"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p41#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 41, "text": "Motif ﬁnding: Searching for structural patterns (1)\nNotations for structural queries:\n●() for vertices, e.g (a), ()-[]->() for edges, e.g (a)-[e]->(b)\n●Anonymous vertices: (), anonymous edges: []\n●Negation of an edge (the edge should not be present in the graph), e.g !(b)-[]->(a)\nPattern/Motif:\n○union of edges, e.g (a)-[e]->(b); (b)-[e2]->(c)\n○Same names for common elements, e.g (a)-[e]->(b); (b)-[e2]->(c)\n○names are used as column names in the result DataFrame"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p42#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 42, "text": "Motif ﬁnding: Searching for structural patterns (2)\nUsage: graph.ﬁnd(\"(a)-[e]->(b); (b)-[e2]->(a)\")\n●Result: a DataFrame with columns for each of the named elements (vertices or edges) in the motif, e.g\na, b, e, e2\n●columns “a” and “b” are StructType with sub-ﬁelds equivalent to the schema of GraphFrame.vertices.\n●column “e” and “e2” are StructType with sub-ﬁelds to the schema of GraphFrame.edges\n●Motifs are not allowed to contain:\n○edges without any named elements (e.g \"()-[]->()\" and \"!()-[]->()\" are prohibited)\n○named edges within negated terms since these named edges would never appear within results,\ne.g \"!(a)-[ab]->(b)\" is invalid, but \"!(a)-[]->(b)\" is valid.\n●Can return duplicate rows, e.g \"(u)-[]->()\" will return a result for each matching edge"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p43#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 43, "text": "Motif ﬁnding: example#Friends of friends\nfof = g.find(\"(x)-[]->(y); (y)-[]->(z);\n!(x)-[]->(z)\").filter(\"x.id != z.id\")\nfof.select(col('x.id').alias('x'), col('y.id').alias('y'),\ncol('z.id').alias('z')).show()\n+---+---+---+\n| x| y| z|\n+---+---+---+\n| a| e| d|\n| d| a| b|\n| d| a| e|\n| a| b| c|\n| e| d| a|\n| f| c| b|\n| e| f| c|\n| a| e| f|\n+---+---+---+\n(x)\n(y)\n(z)"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p44#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 44, "text": "Subgraph (1)\n+---+------+---+\n| id| name|age|\n+---+------+---+\n| a| Alice| 34|\n| b| Bob| 36|\n| e|Esther| 32|\n| f| Fanny| 36|\n+---+------+---+\n# Select subgraph of users older than 30, and\nrelationships of type \"friend\".\ng1 = g.filterVertices(\"age > 30\")\n.filterEdges(\"relationship = 'friend'\")\n.dropIsolatedVertices()"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p45#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 45, "text": "Subgraph (2)\n# Select subgraph based on edges \"e\" of type \"follow\"\n# pointing from a younger user \"a\" to an older user \"b\".\npaths = g.find(\"(a)-[e]->(b)\")\\\n .filter(\"e.relationship = 'follow'\")\\\n .filter(\"a.age < b.age\")\ne2 = paths.select(\"e.src\", \"e.dst\", \"e.relationship\")\n# Construct the subgraph\ng2 = GraphFrame(g.vertices, e2)\ng.triplets.filter(\"src.age < dst.age\").show()\n+----------------+--------------+--------------+\n| src| edge| dst|\n+----------------+--------------+--------------+\n| {d, David, 29}|{d, a, friend}|{a, Alice, 34}|\n| {a, Alice, 34}|{a, b, friend}| {b, Bob, 36}|\n|{c, Charlie, 30}|{c, b, follow}| {b, Bob, 36}|\n| {e, Esther, 32}|{e, f, follow}|{f, Fanny, 36}|\n+----------------+--------------+--------------+"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p46#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 46, "text": "Graph Algorithms●bfs(fromExpr, toExpr, edgeFilter=None, maxPathLength=10)\n○Returns: DataFrame with one Row for each shortest path between matching vertices.\n●connectedComponents(algorithm='graphframes', checkpointInterval=2, broadcastThreshold=1000000)\n○algorithm – connected components algorithm to use (default: “graphframes”) Supported algorithms are\n“graphframes” and “graphx”.\n○checkpointInterval – checkpoint interval in terms of number of iterations (default: 2)\n○broadcastThreshold – broadcast threshold in propagating component assignments (default: 1000000)\n○Returns: DataFrame with new vertices column “component”\n●stronglyConnectedComponents(maxIter): Runs the strongly connected components algorithm on this graph.\nBased on Pregel ()\n●labelPropagation(maxIter): Runs static label propagation for detecting communities in networks. Based on\nPregel, messages sent on edges in both directio"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p46#c1", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 46, "text": "belPropagation(maxIter): Runs static label propagation for detecting communities in networks. Based on\nPregel, messages sent on edges in both directions.\n○maxIter – the number of iterations to run\n○Returns: DataFrame with new vertex column “component”"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p47#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 47, "text": "Graph Algorithms●shortestPaths(landmarks): Runs the shortest path algorithm from a set of landmark vertices in the graph. Takes edge direction into account.○landmarks – a set of one or more landmarks○Returns: DataFrame with new column “distances”\n●triangleCount(): Counts the number of triangles passing through each vertex in this graph.○Returns: DataFrame with new vertex column “count”\n●pageRank(resetProbability=0.15, sourceId=None, maxIter=None, tol=None)○resetProbability – Probability of resetting to a random vertex.○sourceId – (optional) the source vertex for a personalized PageRank.○maxIter – If set, the algorithm is run for a fixed number of iterations. This may not be set if the tol parameter is set.○tol – If set, the algorithm is run until the given tolerance. This may not be set if the numIter parameter is set. (Exactly one of fixed_num_iter or tolerance must be set.)○Returns: Gr"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p47#c1", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 47, "text": " until the given tolerance. This may not be set if the numIter parameter is set. (Exactly one of fixed_num_iter or tolerance must be set.)○Returns: GraphFrame with new vertices column “pagerank” and new edges column “weight”"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p48#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 48, "text": "Graph Algorithms: BFS# Search from \"Esther\" for users of age < 32.\ng.bfs(\"name = 'Esther'\", \"age < 32\",\\\n edgeFilter=\"relationship != 'friend'\", maxPathLength=3).show()\n+---------------+--------------+--------------+--------------+----------------+\n| from| e0| v1| e1| to|\n+---------------+--------------+--------------+--------------+----------------+\n|{e, Esther, 32}|{e, f, follow}|{f, Fanny, 36}|{f, c, follow}|{c, Charlie, 30}|\n+---------------+--------------+--------------+--------------+----------------+"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p49#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 49, "text": "Graph Algorithms: Connected components\ng.connectedComponents().orderBy(\"component\").show()\n+---+-------+---+---------+\n| id| name|age|component|\n+---+-------+---+---------+\n| a| Alice| 34| 0|\n| b| Bob| 36| 0|\n| c|Charlie| 30| 0|\n| d| David| 29| 0|\n| e| Esther| 32| 0|\n| f| Fanny| 36| 0|\n+---+-------+---+---------+"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p50#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 50, "text": "Graph Algorithms: Strongly connected components\ng.stronglyConnectedComponents(maxIter=10).orderBy(\"component\").show()\n+---+-------+---+---------+\n| id| name|age|component|\n+---+-------+---+---------+\n| a| Alice| 34| 0|\n| d| David| 29| 0|\n| e| Esther| 32| 0|\n| b| Bob| 36| 1|\n| c|Charlie| 30| 1|\n| f| Fanny| 36| 5|\n+---+-------+---+---------+"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p51#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 51, "text": "Graph Algorithms: Page Rank\n# Run PageRank until convergence to tolerance \"tol\".\nresults = g.pageRank(resetProbability=0.15, tol=0.01)\nresults.vertices.select(\"id\", \"pagerank\").show()\nresults.edges.select(\"src\", \"dst\", \"weight\").show()\n# Run PageRank personalized for vertex [\"a\", \"b\", \"c\", \"d\"] in parallel\nresults = g.parallelPersonalizedPageRank(resetProbability=0.15, sourceIds=[\"a\", \"b\", \"c\", \"d\"], maxIter=10)"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p52#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 52, "text": "Graph Algorithms: Shortest Paths\ng.shortestPaths(landmarks=[\"a\", \"d\"])\n .select(\"id\", \"distances\").show()"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p53#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 53, "text": "Graph Algorithms: Triangle count\ng.triangleCount().select(\"id\", \"count\").show()"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p54#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 54, "text": "Graph Algorithms: LPA\ng.labelPropagation(maxIter=5).show()\n+---+-------+---+-----+\n| id| name|age|label|\n+---+-------+---+-----+\n| a| Alice| 34| 2|\n| b| Bob| 36| 2|\n| c|Charlie| 30| 1|\n| d| David| 29| 2|\n| e| Esther| 32| 5|\n| f| Fanny| 36| 2|\n+---+-------+---+-----+\n No unique solution"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p55#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 55, "text": "API aggregateMessages\naggregateMessages(aggCol, sendToSrc=None, sendToDst=None)\n●primitive for developing graph algorithms\n●send messages between vertices and aggregate messages for each vertex.\n●When specifying the messages and aggregation function, the user may reference columns using the static methods in\ngraphframes.lib.AggregateMessages.\nParameters:\n●aggCol – the requested aggregation output either as pyspark.sql.Column or SQL expression string\n●sendToSrc – message sent to the source vertex of each triplet either as pyspark.sql.Column or SQL expression string\n(default: None)\n●sendToDst – message sent to the destination vertex of each triplet either as pyspark.sql.Column or SQL expression\nstring (default: None)\nReturns: DataFrame with columns for the vertex ID and the resulting aggregated message"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p56#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 56, "text": "Example: aggregateMessages\nfrom pyspark.sql.functions import sum\nfrom graphframes.lib import AggregateMessages as AM\n# For each user, sum the ages of the adjacent users.\nmsgToSrc = AM.dst[\"age\"]\nmsgToDst = AM.src[\"age\"]\nagg = g.aggregateMessages(\n sum(AM.msg).alias(\"summedAges\"),\n sendToSrc=msgToSrc,\n sendToDst=msgToDst)\nmsgT oSrc=36\n(f →e)\nmsgT oDst=\n34\n(a →e)\nmsgT oSrc=\n29\n(d →e)"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p57#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 57, "text": "Example: aggregateMessages\nfrom pyspark.sql.functions import sum\nfrom graphframes.lib import AggregateMessages as AM\n# For each user, sum the ages of the adjacent users.\nmsgToSrc = AM.dst[\"age\"]\nmsgToDst = AM.src[\"age\"]\nagg = g.aggregateMessages(\n sum(AM.msg).alias(\"summedAges\"),\n sendToSrc=msgToSrc,\n sendToDst=msgToDst)\nsummedAges=36+\n (f →e)\n34\n(a →e)\n29+\n(d →e)"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p58#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 58, "text": "API Pregelclass graphframes.lib.Pregel(graph)\n●implements a Pregel-like bulk-synchronous message-passing (BSP) API based on DataFrame operations.\n●Iterative algorithm that computes the properties of vertices based on the properties of their neighbours\n●returns a DataFrame of vertices from the last iteration.\n●When a run starts, it expands the vertices DataFrame using column expressions defined by withVertexColumn().\n●Three phases for each iteration:\n○For each edge triplet generate messages and specify target vertices to send, described by sendMsgToDst() and\nsendMsgToSrc().\n○Aggregate messages by target vertex IDs, described by aggMsgs()\n○Update additional vertex properties based on aggregated messages and states from previous iteration, described by\nwithVertexColumn().\n●End of computation: when there are no more messages (at each stage, vertices that have not received a message do not se"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p58#c1", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 58, "text": "cribed by\nwithVertexColumn().\n●End of computation: when there are no more messages (at each stage, vertices that have not received a message do not send\nmessages) or the maximum number of iterations has been reached\n○setMaxIter(): control the number of iterations"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p59#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 59, "text": "Example: PageRank computation\nalpha = 0.15\nnumVertices = g.vertices.count()\nranks = g.pregel \\\n .setMaxIter(5) \\\n .withVertexColumn(\"rank\", lit(1.0 / numVertices), \\\n coalesce(Pregel.msg(), lit(0.0)) * lit(1.0 - alpha) + lit(alpha /\nnumVertices)) \\\n .sendMsgToDst(Pregel.src(\"rank\") / Pregel.src(\"outDegree\")) \\\n .aggMsgs(sum(Pregel.msg())) \\\n .run()"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p60#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 60, "text": "Pagerank: initialization\nalpha = 0.15\nnumVertices = g.vertices.count()\nranks = g.pregel \\\n .setMaxIter(5) \\\n .withVertexColumn(\"rank\", lit(1.0 / numVertices), \\\n coalesce(Pregel.msg(), lit(0.0)) * lit(1.0 - alpha) + lit(alpha / numVertices)) \\\n .sendMsgT oDst(Pregel.src(\"rank\") / Pregel.src(\"outDegree\")) \\\n .aggMsgs(sum(Pregel.msg())) \\\n .run()\n0.17\n0.17\n0.170.17\n0.17\n0.17"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p61#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 61, "text": "Pagerank: First Iteration - send messages\nalpha = 0.15\nnumVertices = g.vertices.count()\nranks = g.pregel \\\n .setMaxIter(5) \\\n .withVertexColumn(\"rank\", lit(1.0 / numVertices), \\\n coalesce(Pregel.msg(), lit(0.0)) * lit(1.0 - alpha) + lit(alpha / numVertices)) \\\n .sendMsgT oDst(Pregel.src(\"rank\") / Pregel.src(\"outDegree\")) \\\n .aggMsgs(sum(Pregel.msg())) \\\n .run()\nPregel.msg() =0.17/2\n (e→d)\n0.17/1\n(f→c)\n0.17/2\n(e→f)\n0.17/1\n(d→a)\n0.17/2\n(a→e)\n0.17/2\n(a→b)\n0.17/1\n(b→c)\n0.17/1\n(c→b)"}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p62#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 62, "text": "Pagerank: First Iteration - aggregate messages\nalpha = 0.15\nnumVertices = g.vertices.count()\nranks = g.pregel \\\n .setMaxIter(5) \\\n .withVertexColumn(\"rank\", lit(1.0 / numVertices), \\\n coalesce(Pregel.msg(), lit(0.0)) * lit(1.0 - alpha) + lit(alpha / numVertices)) \\\n .sendMsgT oDst(Pregel.src(\"rank\") / Pregel.src(\"outDegree\")) \\\n .aggMsgs(sum(Pregel.msg())) \\\n .run()\n0.17/2\n(e→d)\n0.17/1 +\n(f→c)\n0.17/2\n(e→f)\n0.17/1\n(d→a)\n0.17/2\n(a→e)\n0.17/2+\n(a→b)\n0.17/1\n(b→c)0.17/1\n(c→b)\nPregel.msg() ="}
{"id": "Lecture3-GraphFrames-ESILV-25-26.pdf#p63#c0", "doc_id": "Lecture3-GraphFrames-ESILV-25-26.pdf", "page": 63, "text": "Pagerank: First Iteration - update ranks\nalpha = 0.15\nnumVertices = g.vertices.count()\nranks = g.pregel \\\n .setMaxIter(5) \\\n .withVertexColumn(\"rank\", lit(1.0 / numVertices), \\\n coalesce(Pregel.msg(), lit(0.0)) * lit(1.0 - alpha) + lit(alpha / numVertices)) \\\n .sendMsgT oDst(Pregel.src(\"rank\") / Pregel.src(\"outDegree\")) \\\n .aggMsgs(sum(Pregel.msg())) \\\n .run()0.17/2\n(e→d)\n(0.17/1 +\n(f→c)\n0.17/2\n(e→f)\n0.17/1\n(d→a)\n0.17/2\n(a→e)\n0.17/2+\n(a→b)\n0.17/1)*(1.0-0.15)+ 0.15/6\n(b→c)0.17/1\n(c→b)"}
